{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "056eb41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.3.1+cu121\n",
      "Uninstalling torch-2.3.1+cu121:\n",
      "  Successfully uninstalled torch-2.3.1+cu121\n",
      "Found existing installation: torchvision 0.18.1+cu121\n",
      "Uninstalling torchvision-0.18.1+cu121:\n",
      "  Successfully uninstalled torchvision-0.18.1+cu121\n",
      "Found existing installation: torchaudio 2.3.1+cu121\n",
      "Uninstalling torchaudio-2.3.1+cu121:\n",
      "  Successfully uninstalled torchaudio-2.3.1+cu121\n",
      "Found existing installation: transformers 4.41.2\n",
      "Uninstalling transformers-4.41.2:\n",
      "  Successfully uninstalled transformers-4.41.2\n",
      "Found existing installation: datasets 2.19.2\n",
      "Uninstalling datasets-2.19.2:\n",
      "  Successfully uninstalled datasets-2.19.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[31mERROR: pip cache commands can not function since cache is disabled.\u001b[0m\u001b[31m\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/texttable-1.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/igraph-0.11.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.11.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.13a0+0d33366-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.ngc.nvidia.com\n",
      "Collecting torch==2.3.1\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting torchvision==0.18.1\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m144.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting torchaudio==2.3.1\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.1) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.1) (11.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.8.93)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch-tensorrt 2.6.0a0 requires torch>=2.5.0, but you have torch 2.3.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.3.1+cu121 torchaudio-2.3.1+cu121 torchvision-0.18.1+cu121\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/texttable-1.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/igraph-0.11.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.11.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.13a0+0d33366-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers==4.41.2\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets==2.19.2\n",
      "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: rouge-score==0.1.2 in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.9.3)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2) (0.4.5)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (22.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (0.7)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.2) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.2) (3.11.9)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score==0.1.2) (2.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score==0.1.2) (3.9.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score==0.1.2) (1.17.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.2) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.19.2) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.19.2) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2) (2024.8.30)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score==0.1.2) (8.1.7)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: transformers, datasets\n",
      "Successfully installed datasets-2.19.2 transformers-4.41.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/texttable-1.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/igraph-0.11.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.11.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.13a0+0d33366-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio transformers datasets -y\n",
    "!pip cache purge\n",
    "\n",
    "# Install a known-good, compatible stack\n",
    "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers==4.41.2 datasets==2.19.2 rouge-score==0.1.2 scikit-learn matplotlib seaborn numpy tqdm\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6102d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77aa2498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    model_name = \"google/mt5-small\"\n",
    "    languages = [\"english\", \"hindi\", \"punjabi\"]\n",
    "    num_experts = 6\n",
    "    top_k = 3\n",
    "    capacity_factor = 1.25\n",
    "    max_input_length = 512\n",
    "    max_target_length = 128\n",
    "\n",
    "    batch_size = 32 \n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 5\n",
    "    warmup_steps = 2000\n",
    "    weight_decay = 0.01\n",
    "    aux_loss_weight = 0.1\n",
    "    gradient_clip_val = 1.0\n",
    "    \n",
    "    num_beams = 4\n",
    "    generation_max_length = 128\n",
    "    \n",
    "    LOG_INTERVALS = 500\n",
    "    VALIDATION_INTERVALS = 999999\n",
    "    validation_subset_size = 100\n",
    "    max_val_generation_batches = 100\n",
    "    num_workers = 8 \n",
    "\n",
    "    train_size = None\n",
    "    val_size = None\n",
    "    test_size = None\n",
    "\n",
    "    checkpoint_dir = \"./\"\n",
    "    save_every_n_epochs = 1\n",
    "    save_best_only = True\n",
    "    best_metric = \"rougeL\"\n",
    "    patience = 3\n",
    "    min_delta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d1539c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Effective Batch Size: 32\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Effective Batch Size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4d03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint:\n",
    "    def __init__(\n",
    "        self,\n",
    "        checkpoint_dir,\n",
    "        save_best_only=True,\n",
    "        best_metric=\"rouge1\",\n",
    "        patience=None,\n",
    "        min_delta=0.001,\n",
    "    ):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_metric = best_metric\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "        self.best_score = float(\"-inf\")\n",
    "        self.best_epoch = 0\n",
    "        self.patience_counter = 0\n",
    "        self.should_stop = False\n",
    "\n",
    "        self.best_model_dir = os.path.join(checkpoint_dir, \"best_model\")\n",
    "        self.epoch_checkpoints_dir = os.path.join(\n",
    "            checkpoint_dir, \"epoch_checkpoints\")\n",
    "\n",
    "        os.makedirs(self.best_model_dir, exist_ok=True)\n",
    "        os.makedirs(self.epoch_checkpoints_dir, exist_ok=True)\n",
    "\n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        epoch,\n",
    "        train_loss,\n",
    "        val_loss,\n",
    "        rouge_scores,\n",
    "        training_history,\n",
    "    ):\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"rouge_scores\": rouge_scores,\n",
    "            \"training_history\": training_history,\n",
    "            \"config\": config.__dict__,\n",
    "            \"best_score\": self.best_score,\n",
    "            \"best_epoch\": self.best_epoch,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "        # Save epoch-specific checkpoint if not save_best_only\n",
    "        if not self.save_best_only:\n",
    "            epoch_path = os.path.join(\n",
    "                self.epoch_checkpoints_dir, f\"checkpoint_epoch_{epoch}.pt\"\n",
    "            )\n",
    "            torch.save(checkpoint_data, epoch_path)\n",
    "            print(f\"Saved epoch checkpoint to {epoch_path}\")\n",
    "\n",
    "        # Check if this is the best model\n",
    "        current_score = rouge_scores[self.best_metric]\n",
    "        if current_score > self.best_score + self.min_delta:\n",
    "            self.best_score = current_score\n",
    "            self.best_epoch = epoch\n",
    "            self.patience_counter = 0\n",
    "\n",
    "            # Save best model\n",
    "            best_path = os.path.join(self.best_model_dir, \"best_model.pt\")\n",
    "            torch.save(checkpoint_data, best_path)\n",
    "            print(\n",
    "                f\"New best model saved! {self.best_metric}: {current_score:.4f} (epoch {epoch})\"\n",
    "            )\n",
    "\n",
    "            # Also save in HuggingFace format\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            model_to_save.base_model.save_pretrained(\n",
    "                os.path.join(self.best_model_dir, \"hf_model\")\n",
    "            )\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            print(\n",
    "                f\"No improvement. Patience: {self.patience_counter}/{self.patience}\")\n",
    "\n",
    "        if self.patience and self.patience_counter >= self.patience:\n",
    "            self.should_stop = True\n",
    "            print(\n",
    "                f\"Early stopping triggered after {self.patience} epochs without improvement\"\n",
    "            )\n",
    "\n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            \"best_score\": self.best_score,\n",
    "            \"best_epoch\": self.best_epoch,\n",
    "            \"current_epoch\": epoch,\n",
    "            \"patience_counter\": self.patience_counter,\n",
    "            \"training_complete\": False,\n",
    "        }\n",
    "\n",
    "        with open(\n",
    "            os.path.join(self.checkpoint_dir, \"training_metadata.json\"), \"w\"\n",
    "        ) as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path, model, optimizer=None, scheduler=None):\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "\n",
    "        checkpoint = torch.load(\n",
    "            checkpoint_path, map_location=device, weights_only=False\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        if optimizer and \"optimizer_state_dict\" in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "        if scheduler and \"scheduler_state_dict\" in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "        self.best_score = checkpoint.get(\"best_score\", float(\"-inf\"))\n",
    "        self.best_epoch = checkpoint.get(\"best_epoch\", 0)\n",
    "\n",
    "        print(f\"Checkpoint loaded successfully!\")\n",
    "        print(f\"Resuming from epoch {checkpoint['epoch']}\")\n",
    "        print(\n",
    "            f\"Best {self.best_metric} score so far: {self.best_score:.4f} (epoch {self.best_epoch})\"\n",
    "        )\n",
    "\n",
    "        return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df05f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_experts, top_k, capacity_factor):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor\n",
    "\n",
    "        # Gating network - added bias for more routing flexibility\n",
    "        self.gate = nn.Linear(hidden_size, num_experts, bias=True)\n",
    "\n",
    "        # Expert networks (FFN layers)\n",
    "        self.experts = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, hidden_size * 4),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.1),\n",
    "                    nn.Linear(hidden_size * 4, hidden_size),\n",
    "                )\n",
    "                for _ in range(num_experts)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Language expert mapping\n",
    "        self.language_expert_map = {\n",
    "            \"en\": 0, \"hi\": 1, \"pa\": 2, \"shared\": [3, 4, 5]}\n",
    "        \n",
    "        # Track overflow statistics\n",
    "        self.overflow_count = 0\n",
    "        self.total_forward_calls = 0\n",
    "\n",
    "    def forward(self, x, language_ids=None):\n",
    "        batch_size, seq_len, hidden_size = x.shape\n",
    "        x_flat = x.view(-1, hidden_size)  # [batch_size * seq_len, hidden_size]\n",
    "        num_tokens = x_flat.size(0)\n",
    "\n",
    "        # Gating with optional language-aware biasing\n",
    "        gate_logits = self.gate(x_flat)  # [num_tokens, num_experts]\n",
    "        \n",
    "        # Language-aware routing: boost probability for language-specific experts\n",
    "        if language_ids is not None and self.training:\n",
    "            # Expand language_ids to all tokens in sequence\n",
    "            lang_bias = torch.zeros_like(gate_logits)\n",
    "            \n",
    "            for batch_idx, lang_id in enumerate(language_ids):\n",
    "                # Apply bias to all tokens in this sample\n",
    "                start_idx = batch_idx * seq_len\n",
    "                end_idx = start_idx + seq_len\n",
    "                \n",
    "                if lang_id in self.language_expert_map:\n",
    "                    expert_idx = self.language_expert_map[lang_id]\n",
    "                    if isinstance(expert_idx, list):\n",
    "                        # Boost shared experts\n",
    "                        for idx in expert_idx:\n",
    "                            lang_bias[start_idx:end_idx, idx] = 0.5\n",
    "                    else:\n",
    "                        # Strong boost for language-specific expert\n",
    "                        lang_bias[start_idx:end_idx, expert_idx] = 1.0\n",
    "            \n",
    "            gate_logits = gate_logits + lang_bias\n",
    "        \n",
    "        gate_probs = F.softmax(gate_logits, dim=-1)  # [num_tokens, num_experts]\n",
    "\n",
    "        # Top-k gating - vectorized\n",
    "        top_k_probs, top_k_indices = torch.topk(gate_probs, self.top_k, dim=-1)\n",
    "        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)  # Normalize\n",
    "\n",
    "        capacity = int(self.capacity_factor * num_tokens / self.num_experts)\n",
    "\n",
    "        target_dtype = torch.float16 if torch.is_autocast_enabled() else x_flat.dtype\n",
    "        expert_outputs = torch.zeros_like(x_flat, dtype=target_dtype)\n",
    "        expert_usage = torch.zeros(self.num_experts, device=x.device)\n",
    "\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            expert_mask = (top_k_indices == expert_idx).any(dim=-1)\n",
    "            \n",
    "            if not expert_mask.any():\n",
    "                continue\n",
    "            \n",
    "            token_indices = torch.where(expert_mask)[0]\n",
    "            expert_input = x_flat[expert_mask]\n",
    "            \n",
    "            expert_weights = torch.zeros(expert_input.size(0), device=x.device, dtype=target_dtype)\n",
    "            for k_idx in range(self.top_k):\n",
    "                matches = (top_k_indices[token_indices, k_idx] == expert_idx)\n",
    "                expert_weights[matches] += top_k_probs[token_indices[matches], k_idx]\n",
    "            \n",
    "            if expert_input.size(0) > capacity:\n",
    "                _, top_indices = torch.topk(expert_weights, k=capacity, largest=True)\n",
    "                token_indices = token_indices[top_indices]\n",
    "                expert_input = expert_input[top_indices]\n",
    "                expert_weights = expert_weights[top_indices]\n",
    "            \n",
    "            expert_output = self.experts[expert_idx](expert_input)\n",
    "            expert_usage[expert_idx] = expert_input.size(0)\n",
    "            \n",
    "            weighted_output = expert_output * expert_weights.unsqueeze(-1)\n",
    "            expert_outputs.index_add_(0, token_indices, weighted_output)\n",
    "\n",
    "        aux_loss = torch.tensor(0.0, device=x.device)\n",
    "        if self.training:\n",
    "            expert_fraction = expert_usage / num_tokens\n",
    "            uniform_target = 1.0 / self.num_experts\n",
    "            aux_loss = ((expert_fraction - uniform_target) ** 2).sum()\n",
    "\n",
    "        output = expert_outputs.view(batch_size, seq_len, hidden_size)\n",
    "        return output, aux_loss, expert_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23abc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MT5WithMoE(nn.Module):\n",
    "    def __init__(self, base_model, num_experts, top_k, capacity_factor):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.config = base_model.config\n",
    "        self.current_language_ids = None\n",
    "\n",
    "        self._replace_ffn_with_moe(num_experts, top_k, capacity_factor)\n",
    "\n",
    "    def _replace_ffn_with_moe(self, num_experts, top_k, capacity_factor):\n",
    "        hidden_size = self.config.d_model\n",
    "\n",
    "        for layer in self.base_model.encoder.block:\n",
    "            if hasattr(layer.layer[1], \"DenseReluDense\"):\n",
    "                moe_layer_instance = MoELayer(\n",
    "                    hidden_size, num_experts, top_k, capacity_factor\n",
    "                )\n",
    "                layer.layer[1].DenseReluDense = MoEWrapper(moe_layer_instance, self)\n",
    "\n",
    "        for layer in self.base_model.decoder.block:\n",
    "            if hasattr(layer.layer[2], \"DenseReluDense\"):\n",
    "                moe_layer_instance = MoELayer(\n",
    "                    hidden_size, num_experts, top_k, capacity_factor\n",
    "                )\n",
    "                layer.layer[2].DenseReluDense = MoEWrapper(moe_layer_instance, self)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, language_ids=None):\n",
    "        self.current_language_ids = language_ids\n",
    "        \n",
    "        aux_losses = []\n",
    "        expert_usage_stats = defaultdict(list)\n",
    "        hooks = []\n",
    "\n",
    "        def hook_fn(module, input, output):\n",
    "            if isinstance(module, MoELayer):\n",
    "                _, aux_loss, expert_usage = output\n",
    "                aux_losses.append(aux_loss.to(input_ids.device))\n",
    "                expert_usage_stats[id(module)].append(expert_usage)\n",
    "\n",
    "        try:\n",
    "            for module in self.modules():\n",
    "                if isinstance(module, MoELayer):\n",
    "                    hook = module.register_forward_hook(hook_fn)\n",
    "                    hooks.append(hook)\n",
    "\n",
    "            outputs = self.base_model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "            )\n",
    "\n",
    "            if aux_losses:\n",
    "                total_aux_loss = sum(aux_losses) / len(aux_losses)\n",
    "                if hasattr(outputs, \"loss\") and outputs.loss is not None:\n",
    "                    outputs.loss += config.aux_loss_weight * total_aux_loss\n",
    "\n",
    "        finally:\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "            self.current_language_ids = None\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df054ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEWrapper(nn.Module):\n",
    "    def __init__(self, moe_layer, parent_model):\n",
    "        super().__init__()\n",
    "        self.moe_layer = moe_layer\n",
    "        import weakref\n",
    "        self._parent_model = weakref.ref(parent_model)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        parent = self._parent_model()\n",
    "        language_ids = parent.current_language_ids if parent else None\n",
    "        output_tensor, aux_loss, expert_usage = self.moe_layer(x, language_ids=language_ids)\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f18a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualSummarizationDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_input_length, max_target_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "        # Add language tokens\n",
    "        special_tokens = [\"<en>\", \"<hi>\", \"<pa>\"]\n",
    "        self.tokenizer.add_special_tokens(\n",
    "            {\"additional_special_tokens\": special_tokens})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        lang_token = f\"<{item['id'].split('.')[0]}>\"\n",
    "        input_text = f\"{lang_token} summarize: {item['text']}\"\n",
    "        target_text = item[\"target\"]\n",
    "\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_input_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encoding.input_ids.squeeze(),\n",
    "            \"attention_mask\": input_encoding.attention_mask.squeeze(),\n",
    "            \"labels\": target_encoding.input_ids.squeeze(),\n",
    "            \"language\": item[\"id\"].split(\".\")[0],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286ac90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "\n",
    "    DATASET_NAME = \"csebuetnlp/xlsum\"\n",
    "\n",
    "    for lang in config.languages:\n",
    "        print(f\"Loading {lang} data...\")\n",
    "\n",
    "        if config.train_size is None:\n",
    "            train_split = load_dataset(DATASET_NAME, lang, split=\"train\")\n",
    "        else:\n",
    "            train_per_lang = config.train_size // len(config.languages)\n",
    "            train_split = load_dataset(DATASET_NAME, lang, split=f\"train[:{train_per_lang}]\")\n",
    "        \n",
    "        train_items = []\n",
    "        for item in train_split:\n",
    "            train_items.append(\n",
    "                {\n",
    "                    \"text\": item[\"text\"],\n",
    "                    \"target\": item[\"summary\"],\n",
    "                    \"id\": f\"{lang}.{len(train_items)}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if config.val_size is None:\n",
    "            val_split = load_dataset(DATASET_NAME, lang, split=\"validation\")\n",
    "        else:\n",
    "            val_per_lang = config.val_size // len(config.languages)\n",
    "            val_split = load_dataset(DATASET_NAME, lang, split=f\"validation[:{val_per_lang}]\")\n",
    "        \n",
    "        val_items = []\n",
    "        for item in val_split:\n",
    "            val_items.append(\n",
    "                {\n",
    "                    \"text\": item[\"text\"],\n",
    "                    \"target\": item[\"summary\"],\n",
    "                    \"id\": f\"{lang}.{len(val_items)}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if config.test_size is None:\n",
    "            test_split = load_dataset(DATASET_NAME, lang, split=\"test\")\n",
    "        else:\n",
    "            test_per_lang = config.test_size // len(config.languages)\n",
    "            test_split = load_dataset(DATASET_NAME, lang, split=f\"test[:{test_per_lang}]\")\n",
    "        \n",
    "        test_items = []\n",
    "        for item in test_split:\n",
    "            test_items.append(\n",
    "                {\n",
    "                    \"text\": item[\"text\"],\n",
    "                    \"target\": item[\"summary\"],\n",
    "                    \"id\": f\"{lang}.{len(test_items)}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        train_data.extend(train_items)\n",
    "        val_data.extend(val_items)\n",
    "        test_data.extend(test_items)\n",
    "\n",
    "        print(\n",
    "            f\"{lang} - Train: {len(train_items)}, Val: {len(val_items)}, Test: {len(test_items)}\"\n",
    "        )\n",
    "\n",
    "    np.random.shuffle(train_data)\n",
    "    np.random.shuffle(val_data)\n",
    "    np.random.shuffle(test_data)\n",
    "\n",
    "    print(\n",
    "        f\"Total dataset sizes - Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\"\n",
    "    )\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ada54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(\n",
    "        [\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "        rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "        rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": np.mean(rouge1_scores),\n",
    "        \"rouge2\": np.mean(rouge2_scores),\n",
    "        \"rougeL\": np.mean(rougeL_scores),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fec89a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_decode(model, input_ids, attention_mask, labels, tokenizer):\n",
    "    \"\"\"Helper function to generate predictions and decode both predictions and references.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (pred_texts, ref_texts) or (None, None) if generation fails\n",
    "    \"\"\"\n",
    "    import time\n",
    "    try:\n",
    "        model_to_generate = model.module if hasattr(model, \"module\") else model\n",
    "        \n",
    "        # Time the generation phase (beam search)\n",
    "        gen_start = time.time()\n",
    "        generated_ids = model_to_generate.base_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=config.generation_max_length,\n",
    "            num_beams=config.num_beams,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "        gen_time = time.time() - gen_start\n",
    "\n",
    "        # Time the decoding phase\n",
    "        decode_start = time.time()\n",
    "        pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Decode references\n",
    "        labels_copy = labels.clone()\n",
    "        labels_copy[labels_copy == -100] = tokenizer.pad_token_id\n",
    "        ref_texts = tokenizer.batch_decode(labels_copy, skip_special_tokens=True)\n",
    "        decode_time = time.time() - decode_start\n",
    "        \n",
    "        # Print timing for first batch only\n",
    "        if not hasattr(generate_and_decode, '_first_call_done'):\n",
    "            print(f\"[DEBUG] First generation: beam search took {gen_time:.3f}s, decode took {decode_time:.3f}s\")\n",
    "            generate_and_decode._first_call_done = True\n",
    "        \n",
    "        return pred_texts, ref_texts\n",
    "    except RuntimeError as e:\n",
    "        print(f\"[Warning] Generation failed: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def validate(model, val_loader, tokenizer, use_subset=False, subset_size=None):\n",
    "    \"\"\"Runs a validation cycle and returns loss and scores.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to validate\n",
    "        val_loader: Validation data loader\n",
    "        tokenizer: Tokenizer for decoding\n",
    "        use_subset: If True, only validate on a subset of data\n",
    "        subset_size: Number of batches to use for subset validation\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    if use_subset:\n",
    "        print(f\"Running quick validation on {subset_size} batches...\")\n",
    "    else:\n",
    "        print(\"Running full validation...\")\n",
    "    \n",
    "    print(f\"[DEBUG] Setting model to eval mode...\")\n",
    "    eval_start = time.time()\n",
    "    model.eval()\n",
    "    print(f\"[DEBUG] Model eval mode set in {time.time() - eval_start:.3f}s\")\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_predictions = []\n",
    "    val_references = []\n",
    "    batches_processed = 0\n",
    "    \n",
    "    batches_to_process = subset_size if use_subset and subset_size else len(val_loader)\n",
    "    \n",
    "    data_transfer_time = 0\n",
    "    forward_pass_time = 0\n",
    "    generation_time = 0\n",
    "    decode_time = 0\n",
    "\n",
    "    print(f\"[DEBUG] Starting validation loop for {batches_to_process} batches...\")\n",
    "    loop_start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            if batch_idx == 0:\n",
    "                print(f\"[DEBUG] First batch retrieved in {time.time() - loop_start:.3f}s\")\n",
    "            \n",
    "            if use_subset and batch_idx >= batches_to_process:\n",
    "                break\n",
    "            \n",
    "            # Time data transfer\n",
    "            transfer_start = time.time()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            data_transfer_time += time.time() - transfer_start\n",
    "\n",
    "            # Time forward pass\n",
    "            forward_start = time.time()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "            forward_pass_time += time.time() - forward_start\n",
    "\n",
    "            loss = outputs.loss\n",
    "            if loss.dim() > 0:\n",
    "                loss = loss.mean()\n",
    "            val_loss += loss.item()\n",
    "            batches_processed += 1\n",
    "\n",
    "            # Time generation\n",
    "            gen_start = time.time()\n",
    "            pred_texts, ref_texts = generate_and_decode(\n",
    "                model, input_ids, attention_mask, labels, tokenizer\n",
    "            )\n",
    "            generation_time += time.time() - gen_start\n",
    "            \n",
    "            if pred_texts is not None:\n",
    "                val_predictions.extend(pred_texts)\n",
    "                val_references.extend(ref_texts)\n",
    "            \n",
    "            if batch_idx == 0:\n",
    "                print(f\"[DEBUG] First batch completed in {time.time() - loop_start:.3f}s\")\n",
    "\n",
    "    print(f\"\\n[DEBUG] Validation Loop Timings:\")\n",
    "    print(f\"  - Data transfer to GPU: {data_transfer_time:.3f}s ({data_transfer_time/batches_processed:.3f}s per batch)\")\n",
    "    print(f\"  - Forward pass (loss): {forward_pass_time:.3f}s ({forward_pass_time/batches_processed:.3f}s per batch)\")\n",
    "    print(f\"  - Generation (beam search): {generation_time:.3f}s ({generation_time/batches_processed:.3f}s per batch)\")\n",
    "    print(f\"  - Total loop time: {time.time() - loop_start:.3f}s\")\n",
    "    \n",
    "    # Time ROUGE calculation\n",
    "    rouge_start = time.time()\n",
    "    avg_val_loss = val_loss / max(batches_processed, 1)\n",
    "    rouge_scores = calculate_rouge_scores(val_predictions, val_references) if val_predictions else {\n",
    "        \"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0\n",
    "    }\n",
    "    rouge_time = time.time() - rouge_start\n",
    "    print(f\"  - ROUGE calculation: {rouge_time:.3f}s\")\n",
    "\n",
    "    model.train()  # Set model back to training mode\n",
    "    \n",
    "    print(f\"[DEBUG] Total validation time: {time.time() - total_start_time:.3f}s\")\n",
    "    print(f\"[DEBUG] Batches processed: {batches_processed}\")\n",
    "    \n",
    "    return avg_val_loss, rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efcbc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, train_loader, val_loader, tokenizer, resume_from_checkpoint=None\n",
    "):\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay\n",
    "    )\n",
    "\n",
    "    total_steps = len(train_loader) * config.num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Initialize GradScaler with conservative settings for H100\n",
    "    scaler = torch.cuda.amp.GradScaler(\n",
    "        init_scale=2**10,  # Start with lower scale (1024) instead of default 2**16\n",
    "        growth_factor=2.0,\n",
    "        backoff_factor=0.5,\n",
    "        growth_interval=100  # Wait longer before increasing scale\n",
    "    )\n",
    "\n",
    "    # Initialize checkpoint manager\n",
    "    checkpoint_manager = ModelCheckpoint(\n",
    "        checkpoint_dir=config.checkpoint_dir,\n",
    "        save_best_only=config.save_best_only,\n",
    "        best_metric=config.best_metric,\n",
    "        patience=config.patience,\n",
    "        min_delta=config.min_delta,\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_rouge_scores = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    if resume_from_checkpoint:\n",
    "        checkpoint = checkpoint_manager.load_checkpoint(\n",
    "            resume_from_checkpoint, model, optimizer, scheduler\n",
    "        )\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        train_losses = checkpoint[\"training_history\"][\"train_losses\"]\n",
    "        val_losses = checkpoint[\"training_history\"][\"val_losses\"]\n",
    "        val_rouge_scores = checkpoint[\"training_history\"][\"val_rouge_scores\"]\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Batch size: {config.batch_size}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            languages = batch[\"language\"]  # Get language IDs\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                    language_ids=languages  # Pass language IDs for language-aware routing\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                if loss.dim() > 0:\n",
    "                    loss = loss.mean()\n",
    "\n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_val)\n",
    "            \n",
    "            # Optimizer step\n",
    "            scaler.step(optimizer)\n",
    "            scale = scaler.get_scale()\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Check for gradient overflow (only warn after warmup)\n",
    "            skip_step = (scaler.get_scale() < scale)\n",
    "            if skip_step and global_step > config.warmup_steps:\n",
    "                print(f\"[Info] Gradient overflow at step {global_step} - optimizer step skipped, scale adjusted\")\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            batches_processed += 1\n",
    "            global_step += 1\n",
    "\n",
    "            # Intra-epoch validation\n",
    "            if global_step % config.VALIDATION_INTERVALS == 0:\n",
    "                subset_batches = config.validation_subset_size // config.batch_size\n",
    "                avg_val_loss, rouge_scores = validate(\n",
    "                    model, val_loader, tokenizer, use_subset=True, subset_size=subset_batches\n",
    "                )\n",
    "\n",
    "                print(f\"\\n--- Intra-Epoch Validation (Step {global_step}) ---\")\n",
    "                print(f\"Val Loss (subset): {avg_val_loss:.4f}\")\n",
    "                print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}, ROUGE-2: {rouge_scores['rouge2']:.4f}, ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "                print(f\"Current LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "                print(f\"Grad Scale: {scaler.get_scale():.1f}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "            # Log training progress\n",
    "            if global_step % config.LOG_INTERVALS == 0:\n",
    "                print(f\"Step {global_step} | Batch {batch_idx+1}/{len(train_loader)} | Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = epoch_loss / max(batches_processed, 1)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # End-of-epoch validation\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"End-of-Epoch {epoch+1} Validation\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Clear GPU cache before validation\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        model.eval()\n",
    "        eoe_val_loss = 0\n",
    "        eoe_val_predictions = []\n",
    "        eoe_val_references = []\n",
    "        eoe_batches_processed = 0\n",
    "        eoe_generation_batches = 0  # Track batches for generation\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                languages = batch[\"language\"]\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels,\n",
    "                        language_ids=languages  # Pass language IDs\n",
    "                    )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                if loss.dim() > 0:\n",
    "                    loss = loss.mean()\n",
    "                eoe_val_loss += loss.item()\n",
    "                eoe_batches_processed += 1\n",
    "\n",
    "                # Generate predictions only for first few batches (for speed)\n",
    "                if eoe_generation_batches < config.max_val_generation_batches:\n",
    "                    pred_texts, ref_texts = generate_and_decode(\n",
    "                        model, input_ids, attention_mask, labels, tokenizer\n",
    "                    )\n",
    "                    \n",
    "                    if pred_texts is not None:\n",
    "                        eoe_val_predictions.extend(pred_texts)\n",
    "                        eoe_val_references.extend(ref_texts)\n",
    "                    eoe_generation_batches += 1\n",
    "\n",
    "        avg_eoe_val_loss = eoe_val_loss / max(eoe_batches_processed, 1)\n",
    "        val_losses.append(avg_eoe_val_loss)\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        eoe_rouge_scores = calculate_rouge_scores(\n",
    "            eoe_val_predictions, eoe_val_references\n",
    "        ) if eoe_val_predictions else {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "        \n",
    "        val_rouge_scores.append(eoe_rouge_scores)\n",
    "\n",
    "        training_history = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"val_rouge_scores\": val_rouge_scores,\n",
    "            \"training_time\": time.time() - start_time,\n",
    "        }\n",
    "\n",
    "        print(f\"\\n--- Epoch {epoch+1} Summary ---\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {avg_eoe_val_loss:.4f}\")\n",
    "        print(f\"ROUGE-1: {eoe_rouge_scores['rouge1']:.4f}, ROUGE-2: {eoe_rouge_scores['rouge2']:.4f}, ROUGE-L: {eoe_rouge_scores['rougeL']:.4f}\")\n",
    "        print(f\"Elapsed Time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % config.save_every_n_epochs == 0:\n",
    "            checkpoint_manager.save_checkpoint(\n",
    "                model,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                epoch,\n",
    "                avg_train_loss,\n",
    "                avg_eoe_val_loss,\n",
    "                eoe_rouge_scores,\n",
    "                training_history,\n",
    "            )\n",
    "\n",
    "        # Check for early stopping\n",
    "        if checkpoint_manager.should_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Training complete\n",
    "    final_training_time = time.time() - start_time\n",
    "    training_history[\"training_time\"] = final_training_time\n",
    "    training_history[\"training_complete\"] = True\n",
    "\n",
    "    # Save final checkpoint\n",
    "    checkpoint_manager.save_checkpoint(\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        epoch,\n",
    "        avg_train_loss,\n",
    "        avg_eoe_val_loss,\n",
    "        eoe_rouge_scores,\n",
    "        training_history,\n",
    "    )\n",
    "\n",
    "    # Update metadata\n",
    "    metadata = {\n",
    "        \"best_score\": checkpoint_manager.best_score,\n",
    "        \"best_epoch\": checkpoint_manager.best_epoch,\n",
    "        \"final_epoch\": epoch,\n",
    "        \"training_complete\": True,\n",
    "        \"total_training_time\": final_training_time,\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(config.checkpoint_dir, \"training_metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Completed!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best {config.best_metric}: {checkpoint_manager.best_score:.4f} (Epoch {checkpoint_manager.best_epoch})\")\n",
    "    print(f\"Total Time: {final_training_time/60:.1f} minutes\")\n",
    "    print(f\"Best model saved: {checkpoint_manager.best_model_dir}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b9763de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, tokenizer):\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    language_performance = {\"en\": {\"preds\": [], \"refs\": []}, \n",
    "                           \"hi\": {\"preds\": [], \"refs\": []}, \n",
    "                           \"pa\": {\"preds\": [], \"refs\": []}}\n",
    "    \n",
    "    print(\"Starting evaluation on test set...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            languages = batch[\"language\"]\n",
    "            \n",
    "            # Generate predictions with error handling\n",
    "            pred_texts, ref_texts = generate_and_decode(\n",
    "                model, input_ids, attention_mask, labels, tokenizer\n",
    "            )\n",
    "            \n",
    "            if pred_texts is None:\n",
    "                print(f\"[Warning] Skipping batch {batch_idx} due to generation error\")\n",
    "                continue\n",
    "            \n",
    "            all_predictions.extend(pred_texts)\n",
    "            all_references.extend(ref_texts)\n",
    "            \n",
    "            # Track per-language performance\n",
    "            for pred, ref, lang in zip(pred_texts, ref_texts, languages):\n",
    "                if lang in language_performance:\n",
    "                    language_performance[lang][\"preds\"].append(pred)\n",
    "                    language_performance[lang][\"refs\"].append(ref)\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {batch_idx + 1}/{len(test_loader)} batches\")\n",
    "    \n",
    "    # Calculate overall ROUGE scores\n",
    "    overall_rouge = calculate_rouge_scores(all_predictions, all_references)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Overall Test Results:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ROUGE-1: {overall_rouge['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {overall_rouge['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {overall_rouge['rougeL']:.4f}\")\n",
    "    \n",
    "    # Calculate per-language ROUGE scores\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Per-Language Test Results:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    language_rouge = {}\n",
    "    for lang, data in language_performance.items():\n",
    "        if data[\"preds\"]:\n",
    "            lang_rouge = calculate_rouge_scores(data[\"preds\"], data[\"refs\"])\n",
    "            language_rouge[lang] = lang_rouge\n",
    "            print(f\"\\n{lang.upper()}:\")\n",
    "            print(f\"  ROUGE-1: {lang_rouge['rouge1']:.4f}\")\n",
    "            print(f\"  ROUGE-2: {lang_rouge['rouge2']:.4f}\")\n",
    "            print(f\"  ROUGE-L: {lang_rouge['rougeL']:.4f}\")\n",
    "            print(f\"  Samples: {len(data['preds'])}\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"overall\": overall_rouge,\n",
    "        \"per_language\": language_rouge\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49f02f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(model, tokenizer):\n",
    "    best_model_path = os.path.join(\n",
    "        config.checkpoint_dir, \"best_model\", \"best_model.pt\")\n",
    "\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(f\"Loading best model from {best_model_path}\")\n",
    "        checkpoint = torch.load(\n",
    "            best_model_path, map_location=device, weights_only=False\n",
    "        )\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        print(f\"Best model loaded successfully!\")\n",
    "        print(\n",
    "            f\"Best {config.best_metric} score: {checkpoint['rouge_scores'][config.best_metric]:.4f}\"\n",
    "        )\n",
    "        print(f\"From epoch: {checkpoint['epoch']}\")\n",
    "        return checkpoint\n",
    "    else:\n",
    "        print(f\"No best model found at {best_model_path}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def plot_training_curves(training_history):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))  # Changed to 1x2\n",
    "\n",
    "    # Training and validation loss\n",
    "    axes[0].plot(training_history[\"train_losses\"], label=\"Train Loss\")\n",
    "    axes[0].plot(training_history[\"val_losses\"], label=\"Val Loss\")\n",
    "    axes[0].set_title(\"Training and Validation Loss\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # ROUGE scores over epochs\n",
    "    rouge_scores = training_history[\"val_rouge_scores\"]\n",
    "    epochs = range(1, len(rouge_scores) + 1)\n",
    "\n",
    "    axes[1].plot(epochs, [s[\"rouge1\"] for s in rouge_scores], label=\"ROUGE-1\")\n",
    "    axes[1].plot(epochs, [s[\"rouge2\"] for s in rouge_scores], label=\"ROUGE-2\")\n",
    "    axes[1].plot(epochs, [s[\"rougeL\"] for s in rouge_scores], label=\"ROUGE-L\")\n",
    "    axes[1].set_title(\"Validation ROUGE Scores\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"ROUGE Score\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_expert_usage(model):\n",
    "    \"\"\"Visualize expert usage statistics across MoE layers.\"\"\"\n",
    "    expert_overflow_rates = []\n",
    "    expert_names = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, MoELayer):\n",
    "            expert_names.append(name)\n",
    "            overflow_rate = module.overflow_count / max(module.total_forward_calls, 1)\n",
    "            expert_overflow_rates.append(overflow_rate * 100)\n",
    "    \n",
    "    if expert_overflow_rates:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        bars = ax.bar(range(len(expert_names)), expert_overflow_rates, color='coral')\n",
    "        ax.set_xlabel(\"MoE Layer\")\n",
    "        ax.set_ylabel(\"Overflow Rate (%)\")\n",
    "        ax.set_title(\"Expert Capacity Overflow Rates by Layer\")\n",
    "        ax.set_xticks(range(len(expert_names)))\n",
    "        ax.set_xticklabels([f\"Layer {i}\" for i in range(len(expert_names))], rotation=45)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.1f}%', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        avg_overflow = np.mean(expert_overflow_rates)\n",
    "        print(f\"\\n--- Expert Usage Summary ---\")\n",
    "        print(f\"Average overflow rate: {avg_overflow:.2f}%\")\n",
    "        print(f\"Max overflow rate: {max(expert_overflow_rates):.2f}%\")\n",
    "        print(f\"Layers with overflow: {sum(1 for rate in expert_overflow_rates if rate > 0)}/{len(expert_overflow_rates)}\")\n",
    "    else:\n",
    "        print(\"No MoE layers found for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea16366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_language_performance(language_rouge):\n",
    "    languages = list(language_rouge.keys())\n",
    "    rouge1_scores = [language_rouge[lang][\"rouge1\"] for lang in languages]\n",
    "    rouge2_scores = [language_rouge[lang][\"rouge2\"] for lang in languages]\n",
    "    rougeL_scores = [language_rouge[lang][\"rougeL\"] for lang in languages]\n",
    "\n",
    "    x = np.arange(len(languages))\n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(x - width, rouge1_scores, width, label=\"ROUGE-1\")\n",
    "    ax.bar(x, rouge2_scores, width, label=\"ROUGE-2\")\n",
    "    ax.bar(x + width, rougeL_scores, width, label=\"ROUGE-L\")\n",
    "\n",
    "    ax.set_xlabel(\"Languages\")\n",
    "    ax.set_ylabel(\"ROUGE Scores\")\n",
    "    ax.set_title(\"Language-Specific Performance\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(languages)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01d34810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since csebuetnlp/xlsum couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'english' at /root/.cache/huggingface/datasets/csebuetnlp___xlsum/english/2.0.0/30fece425f9a3866e04321773ca7a80056d55ca6 (last modified on Sat Nov  1 15:37:46 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading english data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since csebuetnlp/xlsum couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'english' at /root/.cache/huggingface/datasets/csebuetnlp___xlsum/english/2.0.0/30fece425f9a3866e04321773ca7a80056d55ca6 (last modified on Sat Nov  1 15:37:46 2025).\n",
      "Using the latest cached version of the dataset since csebuetnlp/xlsum couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'english' at /root/.cache/huggingface/datasets/csebuetnlp___xlsum/english/2.0.0/30fece425f9a3866e04321773ca7a80056d55ca6 (last modified on Sat Nov  1 15:37:46 2025).\n",
      "Using the latest cached version of the dataset since csebuetnlp/xlsum couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'hindi' at /root/.cache/huggingface/datasets/csebuetnlp___xlsum/hindi/2.0.0/30fece425f9a3866e04321773ca7a80056d55ca6 (last modified on Sat Nov  1 15:38:18 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english - Train: 306522, Val: 11535, Test: 11535\n",
      "Loading hindi data...\n",
      "hindi - Train: 70778, Val: 8847, Test: 8847\n",
      "Loading punjabi data...\n",
      "punjabi - Train: 8215, Val: 1026, Test: 1026\n",
      "Total dataset sizes - Train: 385515, Val: 21408, Test: 21408\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "train_data, val_data, test_data = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cad9928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and base model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer and base model...\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(config.model_name, legacy=False)\n",
    "base_model = MT5ForConditionalGeneration.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06c8b9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared successfully\n",
      "GPU memory cleanup complete\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Delete existing model if it exists\n",
    "try:\n",
    "    if 'model' in dir():\n",
    "        del model\n",
    "        print(\"Deleted existing model from memory\")\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# Try to clear GPU cache (may fail if GPU is in error state)\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"GPU cache cleared successfully\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU Error: {str(e)}\")\n",
    "        print(\"\\nSOLUTION: Restart the kernel to clear the GPU error state\")\n",
    "        print(\"   1. Click 'Kernel' menu → 'Restart Kernel'\")\n",
    "        print(\"   2. Re-run cells from the beginning\")\n",
    "        raise\n",
    "    \n",
    "# Garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"GPU memory cleanup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99669ff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MoE model for a single GPU...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MT5WithMoE(\n",
       "  (base_model): MT5ForConditionalGeneration(\n",
       "    (shared): Embedding(250112, 512)\n",
       "    (encoder): MT5Stack(\n",
       "      (embed_tokens): Embedding(250112, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): MT5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): MT5LayerSelfAttention(\n",
       "              (SelfAttention): MT5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 6)\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): MT5LayerFF(\n",
       "              (DenseReluDense): MoEWrapper(\n",
       "                (moe_layer): MoELayer(\n",
       "                  (gate): Linear(in_features=512, out_features=6, bias=True)\n",
       "                  (experts): ModuleList(\n",
       "                    (0-5): 6 x Sequential(\n",
       "                      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (1): ReLU()\n",
       "                      (2): Dropout(p=0.1, inplace=False)\n",
       "                      (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-7): 7 x MT5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): MT5LayerSelfAttention(\n",
       "              (SelfAttention): MT5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): MT5LayerFF(\n",
       "              (DenseReluDense): MoEWrapper(\n",
       "                (moe_layer): MoELayer(\n",
       "                  (gate): Linear(in_features=512, out_features=6, bias=True)\n",
       "                  (experts): ModuleList(\n",
       "                    (0-5): 6 x Sequential(\n",
       "                      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (1): ReLU()\n",
       "                      (2): Dropout(p=0.1, inplace=False)\n",
       "                      (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): MT5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): MT5Stack(\n",
       "      (embed_tokens): Embedding(250112, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): MT5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): MT5LayerSelfAttention(\n",
       "              (SelfAttention): MT5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 6)\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): MT5LayerCrossAttention(\n",
       "              (EncDecAttention): MT5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): MT5LayerFF(\n",
       "              (DenseReluDense): MoEWrapper(\n",
       "                (moe_layer): MoELayer(\n",
       "                  (gate): Linear(in_features=512, out_features=6, bias=True)\n",
       "                  (experts): ModuleList(\n",
       "                    (0-5): 6 x Sequential(\n",
       "                      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (1): ReLU()\n",
       "                      (2): Dropout(p=0.1, inplace=False)\n",
       "                      (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-7): 7 x MT5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): MT5LayerSelfAttention(\n",
       "              (SelfAttention): MT5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): MT5LayerCrossAttention(\n",
       "              (EncDecAttention): MT5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): MT5LayerFF(\n",
       "              (DenseReluDense): MoEWrapper(\n",
       "                (moe_layer): MoELayer(\n",
       "                  (gate): Linear(in_features=512, out_features=6, bias=True)\n",
       "                  (experts): ModuleList(\n",
       "                    (0-5): 6 x Sequential(\n",
       "                      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (1): ReLU()\n",
       "                      (2): Dropout(p=0.1, inplace=False)\n",
       "                      (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): MT5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): MT5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Setting up MoE model for a single GPU...\")\n",
    "model = MT5WithMoE(\n",
    "    base_model, config.num_experts, config.top_k, config.capacity_factor\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efbc1b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing token embeddings...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(250100, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Resizing token embeddings...\")\n",
    "model.base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a37ec9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets and dataloaders...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating datasets and dataloaders...\")\n",
    "train_dataset = MultilingualSummarizationDataset(\n",
    "    tokenizer, train_data, config.max_input_length, config.max_target_length\n",
    ")\n",
    "val_dataset = MultilingualSummarizationDataset(\n",
    "    tokenizer, val_data, config.max_input_length, config.max_target_length\n",
    ")\n",
    "test_dataset = MultilingualSummarizationDataset(\n",
    "    tokenizer, test_data, config.max_input_length, config.max_target_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cec9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b4bcd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 476,620,256\n",
      "Trainable parameters: 476,620,256\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\n",
    "    f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ee43160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEBUG: Testing validation speed on small subset\n",
      "================================================================================\n",
      "\n",
      "[TEST] Running validate() on 2 batches...\n",
      "Running quick validation on 2 batches...\n",
      "[DEBUG] Setting model to eval mode...\n",
      "[DEBUG] Model eval mode set in 0.001s\n",
      "[DEBUG] Starting validation loop for 2 batches...\n",
      "[DEBUG] First batch retrieved in 0.945s\n",
      "[DEBUG] First generation: beam search took 5.431s, decode took 0.099s\n",
      "[DEBUG] First batch completed in 7.182s\n",
      "\n",
      "[DEBUG] Validation Loop Timings:\n",
      "  - Data transfer to GPU: 0.001s (0.000s per batch)\n",
      "  - Forward pass (loss): 0.811s (0.406s per batch)\n",
      "  - Generation (beam search): 10.722s (5.361s per batch)\n",
      "  - Total loop time: 12.655s\n",
      "  - ROUGE calculation: 0.007s\n",
      "[DEBUG] Total validation time: 12.665s\n",
      "[DEBUG] Batches processed: 2\n",
      "\n",
      "[TEST] Results:\n",
      "  - Val Loss: 32.6713\n",
      "  - ROUGE-1: 0.0000\n",
      "  - ROUGE-2: 0.0000\n",
      "  - ROUGE-L: 0.0000\n",
      "\n",
      "[TEST] Total time: 12.666s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEBUG: Testing validation speed on small subset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test with just 2 batches to identify bottleneck\n",
    "import time\n",
    "overall_start = time.time()\n",
    "\n",
    "# Reset the first call flag for generate_and_decode\n",
    "if hasattr(generate_and_decode, '_first_call_done'):\n",
    "    delattr(generate_and_decode, '_first_call_done')\n",
    "\n",
    "print(\"\\n[TEST] Running validate() on 2 batches...\")\n",
    "test_val_loss, test_rouge = validate(model, val_loader, tokenizer, use_subset=True, subset_size=2)\n",
    "\n",
    "print(f\"\\n[TEST] Results:\")\n",
    "print(f\"  - Val Loss: {test_val_loss:.4f}\")\n",
    "print(f\"  - ROUGE-1: {test_rouge['rouge1']:.4f}\")\n",
    "print(f\"  - ROUGE-2: {test_rouge['rouge2']:.4f}\")\n",
    "print(f\"  - ROUGE-L: {test_rouge['rougeL']:.4f}\")\n",
    "print(f\"\\n[TEST] Total time: {time.time() - overall_start:.3f}s\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef71375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Training ---\n",
      "Starting training...\n",
      "Batch size: 32\n",
      "\n",
      "============================================================\n",
      "Epoch 1/5\n",
      "============================================================\n",
      "Step 500 | Batch 500/12048 | Train Loss: 2.2441\n",
      "Step 1000 | Batch 1000/12048 | Train Loss: 1.4841\n",
      "Step 1500 | Batch 1500/12048 | Train Loss: 1.4296\n",
      "Step 2000 | Batch 2000/12048 | Train Loss: 1.3498\n",
      "[Info] Gradient overflow at step 2107 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 2237 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 2365 - optimizer step skipped, scale adjusted\n",
      "Step 2500 | Batch 2500/12048 | Train Loss: 1.2731\n",
      "[Info] Gradient overflow at step 2575 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 2677 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 2786 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 2896 - optimizer step skipped, scale adjusted\n",
      "Step 3000 | Batch 3000/12048 | Train Loss: 1.1514\n",
      "[Info] Gradient overflow at step 3016 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 3217 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 3231 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 3248 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 3458 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 3463 - optimizer step skipped, scale adjusted\n",
      "Step 3500 | Batch 3500/12048 | Train Loss: 1.3074\n",
      "[Info] Gradient overflow at step 3667 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 3788 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 3989 - optimizer step skipped, scale adjusted\n",
      "Step 4000 | Batch 4000/12048 | Train Loss: 0.9674\n",
      "[Info] Gradient overflow at step 4092 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 4177 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 4378 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 4479 - optimizer step skipped, scale adjusted\n",
      "Step 4500 | Batch 4500/12048 | Train Loss: 1.1156\n",
      "[Info] Gradient overflow at step 4581 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 4682 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 4791 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 4886 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 4923 - optimizer step skipped, scale adjusted\n",
      "Step 5000 | Batch 5000/12048 | Train Loss: 1.0858\n",
      "[Info] Gradient overflow at step 5036 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 5352 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 5453 - optimizer step skipped, scale adjusted\n",
      "Step 5500 | Batch 5500/12048 | Train Loss: 0.9494\n",
      "[Info] Gradient overflow at step 5573 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 5620 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 5833 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 5862 - optimizer step skipped, scale adjusted\n",
      "Step 6000 | Batch 6000/12048 | Train Loss: 0.9521\n",
      "[Info] Gradient overflow at step 6078 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 6203 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 6338 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 6380 - optimizer step skipped, scale adjusted\n",
      "Step 6500 | Batch 6500/12048 | Train Loss: 0.9865\n",
      "[Info] Gradient overflow at step 6585 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 6697 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 6804 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 6989 - optimizer step skipped, scale adjusted\n",
      "Step 7000 | Batch 7000/12048 | Train Loss: 1.1275\n",
      "[Info] Gradient overflow at step 7110 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 7228 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 7352 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 7354 - optimizer step skipped, scale adjusted\n",
      "Step 7500 | Batch 7500/12048 | Train Loss: 0.9847\n",
      "[Info] Gradient overflow at step 7540 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 7747 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 7832 - optimizer step skipped, scale adjusted\n",
      "Step 8000 | Batch 8000/12048 | Train Loss: 0.9318\n",
      "[Info] Gradient overflow at step 8035 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 8158 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 8311 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 8412 - optimizer step skipped, scale adjusted\n",
      "Step 8500 | Batch 8500/12048 | Train Loss: 0.9873\n",
      "[Info] Gradient overflow at step 8542 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 8597 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 8601 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 8908 - optimizer step skipped, scale adjusted\n",
      "Step 9000 | Batch 9000/12048 | Train Loss: 1.0474\n",
      "[Info] Gradient overflow at step 9109 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 9146 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 9243 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 9248 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 9251 - optimizer step skipped, scale adjusted\n",
      "Step 9500 | Batch 9500/12048 | Train Loss: 0.9266\n",
      "[Info] Gradient overflow at step 9658 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 9770 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 9851 - optimizer step skipped, scale adjusted\n",
      "Step 10000 | Batch 10000/12048 | Train Loss: 0.9107\n",
      "[Info] Gradient overflow at step 10103 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 10128 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 10369 - optimizer step skipped, scale adjusted\n",
      "Step 10500 | Batch 10500/12048 | Train Loss: 0.9693\n",
      "[Info] Gradient overflow at step 10500 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 10612 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 10813 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 10822 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 10934 - optimizer step skipped, scale adjusted\n",
      "Step 11000 | Batch 11000/12048 | Train Loss: 0.7479\n",
      "[Info] Gradient overflow at step 11124 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 11315 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 11435 - optimizer step skipped, scale adjusted\n",
      "Step 11500 | Batch 11500/12048 | Train Loss: 0.7771\n",
      "[Info] Gradient overflow at step 11596 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 11797 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 11802 - optimizer step skipped, scale adjusted\n",
      "Step 12000 | Batch 12000/12048 | Train Loss: 1.0787\n",
      "[Info] Gradient overflow at step 12007 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 12011 - optimizer step skipped, scale adjusted\n",
      "\n",
      "============================================================\n",
      "End-of-Epoch 1 Validation\n",
      "============================================================\n",
      "\n",
      "--- Epoch 1 Summary ---\n",
      "Train Loss: 1.5088\n",
      "Val Loss: 0.8676\n",
      "ROUGE-1: 0.1449, ROUGE-2: 0.0350, ROUGE-L: 0.1104\n",
      "Elapsed Time: 88.0 minutes\n",
      "============================================================\n",
      "\n",
      "New best model saved! rougeL: 0.1104 (epoch 0)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/5\n",
      "============================================================\n",
      "[Info] Gradient overflow at step 12156 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 12269 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 12385 - optimizer step skipped, scale adjusted\n",
      "Step 12500 | Batch 452/12048 | Train Loss: 1.0333\n",
      "[Info] Gradient overflow at step 12586 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 12628 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 12800 - optimizer step skipped, scale adjusted\n",
      "Step 13000 | Batch 952/12048 | Train Loss: 0.9175\n",
      "[Info] Gradient overflow at step 13002 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 13076 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 13278 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 13379 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 13403 - optimizer step skipped, scale adjusted\n",
      "Step 13500 | Batch 1452/12048 | Train Loss: 0.9139\n",
      "[Info] Gradient overflow at step 13575 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 13698 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 13806 - optimizer step skipped, scale adjusted\n",
      "Step 14000 | Batch 1952/12048 | Train Loss: 1.0809\n",
      "[Info] Gradient overflow at step 14007 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 14112 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 14213 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 14316 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 14417 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 14434 - optimizer step skipped, scale adjusted\n",
      "Step 14500 | Batch 2452/12048 | Train Loss: 1.0043\n",
      "[Info] Gradient overflow at step 14638 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 14645 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 14803 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 14909 - optimizer step skipped, scale adjusted\n",
      "Step 15000 | Batch 2952/12048 | Train Loss: 0.9303\n",
      "[Info] Gradient overflow at step 15016 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 15190 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 15393 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 15409 - optimizer step skipped, scale adjusted\n",
      "Step 15500 | Batch 3452/12048 | Train Loss: 0.9589\n",
      "[Info] Gradient overflow at step 15610 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 15711 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 15786 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 15928 - optimizer step skipped, scale adjusted\n",
      "Step 16000 | Batch 3952/12048 | Train Loss: 0.9091\n",
      "[Info] Gradient overflow at step 16043 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 16191 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 16392 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 16406 - optimizer step skipped, scale adjusted\n",
      "Step 16500 | Batch 4452/12048 | Train Loss: 0.9306\n",
      "[Info] Gradient overflow at step 16611 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 16619 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 16820 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 16919 - optimizer step skipped, scale adjusted\n",
      "Step 17000 | Batch 4952/12048 | Train Loss: 0.8843\n",
      "[Info] Gradient overflow at step 17062 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 17195 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 17352 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 17498 - optimizer step skipped, scale adjusted\n",
      "Step 17500 | Batch 5452/12048 | Train Loss: 0.7839\n",
      "[Info] Gradient overflow at step 17699 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 17795 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 17969 - optimizer step skipped, scale adjusted\n",
      "Step 18000 | Batch 5952/12048 | Train Loss: 0.9390\n",
      "[Info] Gradient overflow at step 18170 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 18271 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 18327 - optimizer step skipped, scale adjusted\n",
      "Step 18500 | Batch 6452/12048 | Train Loss: 0.7665\n",
      "[Info] Gradient overflow at step 18530 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 18631 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 18724 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 18926 - optimizer step skipped, scale adjusted\n",
      "Step 19000 | Batch 6952/12048 | Train Loss: 0.7266\n",
      "[Info] Gradient overflow at step 19028 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 19049 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 19250 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 19351 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 19458 - optimizer step skipped, scale adjusted\n",
      "Step 19500 | Batch 7452/12048 | Train Loss: 0.8137\n",
      "[Info] Gradient overflow at step 19554 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 19684 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 19886 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 19956 - optimizer step skipped, scale adjusted\n",
      "Step 20000 | Batch 7952/12048 | Train Loss: 0.8973\n",
      "[Info] Gradient overflow at step 20084 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 20285 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 20386 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 20426 - optimizer step skipped, scale adjusted\n",
      "Step 20500 | Batch 8452/12048 | Train Loss: 0.8056\n",
      "[Info] Gradient overflow at step 20634 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 20740 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 20786 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 20988 - optimizer step skipped, scale adjusted\n",
      "Step 21000 | Batch 8952/12048 | Train Loss: 0.8036\n",
      "[Info] Gradient overflow at step 21014 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 21216 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 21317 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 21372 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 21498 - optimizer step skipped, scale adjusted\n",
      "Step 21500 | Batch 9452/12048 | Train Loss: 0.9226\n",
      "[Info] Gradient overflow at step 21625 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 21812 - optimizer step skipped, scale adjusted\n",
      "Step 22000 | Batch 9952/12048 | Train Loss: 0.7896\n",
      "[Info] Gradient overflow at step 22013 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 22118 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 22221 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 22316 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 22429 - optimizer step skipped, scale adjusted\n",
      "Step 22500 | Batch 10452/12048 | Train Loss: 0.7727\n",
      "[Info] Gradient overflow at step 22531 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 22732 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 22839 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 22944 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 22946 - optimizer step skipped, scale adjusted\n",
      "Step 23000 | Batch 10952/12048 | Train Loss: 0.8518\n",
      "[Info] Gradient overflow at step 23148 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 23250 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 23353 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 23422 - optimizer step skipped, scale adjusted\n",
      "Step 23500 | Batch 11452/12048 | Train Loss: 0.9862\n",
      "[Info] Gradient overflow at step 23623 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 23725 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 23742 - optimizer step skipped, scale adjusted\n",
      "\n",
      "--- Epoch 2 Summary ---\n",
      "Train Loss: 0.8620\n",
      "Val Loss: 0.7882\n",
      "ROUGE-1: 0.1560, ROUGE-2: 0.0427, ROUGE-L: 0.1200\n",
      "Elapsed Time: 175.7 minutes\n",
      "============================================================\n",
      "\n",
      "New best model saved! rougeL: 0.1200 (epoch 1)\n",
      "\n",
      "============================================================\n",
      "Epoch 3/5\n",
      "============================================================\n",
      "[Info] Gradient overflow at step 24176 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 24284 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 24386 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 24491 - optimizer step skipped, scale adjusted\n",
      "Step 24500 | Batch 404/12048 | Train Loss: 0.8196\n",
      "[Info] Gradient overflow at step 24593 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 24696 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 24788 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 24990 - optimizer step skipped, scale adjusted\n",
      "Step 25000 | Batch 904/12048 | Train Loss: 0.8630\n",
      "[Info] Gradient overflow at step 25095 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 25118 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 25241 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 25443 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 25457 - optimizer step skipped, scale adjusted\n",
      "Step 25500 | Batch 1404/12048 | Train Loss: 0.8900\n",
      "[Info] Gradient overflow at step 25583 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 25784 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 25885 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 25893 - optimizer step skipped, scale adjusted\n",
      "Step 26000 | Batch 1904/12048 | Train Loss: 0.7758\n",
      "[Info] Gradient overflow at step 26040 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 26243 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 26257 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 26458 - optimizer step skipped, scale adjusted\n",
      "Step 26500 | Batch 2404/12048 | Train Loss: 0.8871\n",
      "[Info] Gradient overflow at step 26551 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 26687 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 26890 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 26994 - optimizer step skipped, scale adjusted\n",
      "Step 27000 | Batch 2904/12048 | Train Loss: 0.8876\n",
      "[Info] Gradient overflow at step 27057 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 27258 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 27272 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 27386 - optimizer step skipped, scale adjusted\n",
      "Step 27500 | Batch 3404/12048 | Train Loss: 0.8342\n",
      "[Info] Gradient overflow at step 27587 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 27688 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 27795 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 27826 - optimizer step skipped, scale adjusted\n",
      "Step 28000 | Batch 3904/12048 | Train Loss: 0.9228\n",
      "[Info] Gradient overflow at step 28028 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 28054 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 28261 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 28312 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 28619 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 28721 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 28823 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 28926 - optimizer step skipped, scale adjusted\n",
      "Step 29000 | Batch 4904/12048 | Train Loss: 0.7471\n",
      "[Info] Gradient overflow at step 29029 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 29131 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 29232 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 29334 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 29435 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 29467 - optimizer step skipped, scale adjusted\n",
      "Step 29500 | Batch 5404/12048 | Train Loss: 0.6312\n",
      "[Info] Gradient overflow at step 29571 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 29783 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 29802 - optimizer step skipped, scale adjusted\n",
      "Step 30000 | Batch 5904/12048 | Train Loss: 0.7679\n",
      "[Info] Gradient overflow at step 30006 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 30099 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 30301 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 30320 - optimizer step skipped, scale adjusted\n",
      "Step 30500 | Batch 6404/12048 | Train Loss: 0.8581\n",
      "[Info] Gradient overflow at step 30521 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 30623 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 30724 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 30826 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 30929 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 31238 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 31309 - optimizer step skipped, scale adjusted\n",
      "Step 31500 | Batch 7404/12048 | Train Loss: 0.8676\n",
      "[Info] Gradient overflow at step 31510 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 31557 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 31762 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 31869 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 31972 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 31995 - optimizer step skipped, scale adjusted\n",
      "Step 32000 | Batch 7904/12048 | Train Loss: 0.7271\n",
      "[Info] Gradient overflow at step 32197 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 32298 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 32399 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 32409 - optimizer step skipped, scale adjusted\n",
      "Step 32500 | Batch 8404/12048 | Train Loss: 0.6940\n",
      "[Info] Gradient overflow at step 32610 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 32633 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 32835 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 32943 - optimizer step skipped, scale adjusted\n",
      "Step 33000 | Batch 8904/12048 | Train Loss: 0.8055\n",
      "[Info] Gradient overflow at step 33047 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 33148 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 33209 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 33413 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 33460 - optimizer step skipped, scale adjusted\n",
      "Step 33500 | Batch 9404/12048 | Train Loss: 0.8257\n",
      "[Info] Gradient overflow at step 33661 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 33763 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 33866 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 33968 - optimizer step skipped, scale adjusted\n",
      "Step 34000 | Batch 9904/12048 | Train Loss: 0.7704\n",
      "[Info] Gradient overflow at step 34069 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 34100 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 34256 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 34288 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 34290 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 34292 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 34293 - optimizer step skipped, scale adjusted\n",
      "Step 34500 | Batch 10404/12048 | Train Loss: 0.6541\n",
      "[Info] Gradient overflow at step 34811 - optimizer step skipped, scale adjusted\n",
      "Step 35000 | Batch 10904/12048 | Train Loss: 0.8951\n",
      "[Info] Gradient overflow at step 35013 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 35073 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 35276 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 35377 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 35480 - optimizer step skipped, scale adjusted\n",
      "Step 35500 | Batch 11404/12048 | Train Loss: 0.8358\n",
      "[Info] Gradient overflow at step 35584 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 35685 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 35786 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 35887 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 35971 - optimizer step skipped, scale adjusted\n",
      "Step 36000 | Batch 11904/12048 | Train Loss: 0.8709\n",
      "[Info] Gradient overflow at step 36143 - optimizer step skipped, scale adjusted\n",
      "\n",
      "============================================================\n",
      "End-of-Epoch 3 Validation\n",
      "============================================================\n",
      "\n",
      "--- Epoch 3 Summary ---\n",
      "Train Loss: 0.7958\n",
      "Val Loss: 0.7487\n",
      "ROUGE-1: 0.1617, ROUGE-2: 0.0463, ROUGE-L: 0.1259\n",
      "Elapsed Time: 262.9 minutes\n",
      "============================================================\n",
      "\n",
      "New best model saved! rougeL: 0.1259 (epoch 2)\n",
      "\n",
      "============================================================\n",
      "Epoch 4/5\n",
      "============================================================\n",
      "[Info] Gradient overflow at step 36346 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 36449 - optimizer step skipped, scale adjusted\n",
      "Step 36500 | Batch 356/12048 | Train Loss: 0.6957\n",
      "[Info] Gradient overflow at step 36550 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 36652 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 36673 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 36811 - optimizer step skipped, scale adjusted\n",
      "Step 37000 | Batch 856/12048 | Train Loss: 0.7472\n",
      "[Info] Gradient overflow at step 37013 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 37115 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 37132 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 37256 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 37400 - optimizer step skipped, scale adjusted\n",
      "Step 37500 | Batch 1356/12048 | Train Loss: 0.7456\n",
      "[Info] Gradient overflow at step 37542 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 37687 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 37887 - optimizer step skipped, scale adjusted\n",
      "Step 38000 | Batch 1856/12048 | Train Loss: 0.7577\n",
      "[Info] Gradient overflow at step 38088 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 38193 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 38294 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 38351 - optimizer step skipped, scale adjusted\n",
      "Step 38500 | Batch 2356/12048 | Train Loss: 0.7150\n",
      "[Info] Gradient overflow at step 38558 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 38661 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 38763 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 38866 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 38967 - optimizer step skipped, scale adjusted\n",
      "Step 39000 | Batch 2856/12048 | Train Loss: 0.7907\n",
      "[Info] Gradient overflow at step 39068 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 39173 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 39278 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 39347 - optimizer step skipped, scale adjusted\n",
      "Step 39500 | Batch 3356/12048 | Train Loss: 0.7142\n",
      "[Info] Gradient overflow at step 39554 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 39602 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 39731 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 39933 - optimizer step skipped, scale adjusted\n",
      "Step 40000 | Batch 3856/12048 | Train Loss: 0.7412\n",
      "[Info] Gradient overflow at step 40043 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 40144 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 40245 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 40346 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 40448 - optimizer step skipped, scale adjusted\n",
      "Step 40500 | Batch 4356/12048 | Train Loss: 0.7716\n",
      "[Info] Gradient overflow at step 40549 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 40614 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 40717 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 40918 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 40936 - optimizer step skipped, scale adjusted\n",
      "Step 41000 | Batch 4856/12048 | Train Loss: 0.7536\n",
      "[Info] Gradient overflow at step 41139 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 41240 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 41345 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 41429 - optimizer step skipped, scale adjusted\n",
      "Step 41500 | Batch 5356/12048 | Train Loss: 0.8419\n",
      "[Info] Gradient overflow at step 41631 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 41687 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 41888 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 41989 - optimizer step skipped, scale adjusted\n",
      "Step 42000 | Batch 5856/12048 | Train Loss: 0.9285\n",
      "[Info] Gradient overflow at step 42048 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 42191 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 42396 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 42472 - optimizer step skipped, scale adjusted\n",
      "Step 42500 | Batch 6356/12048 | Train Loss: 0.7431\n",
      "[Info] Gradient overflow at step 42676 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 42779 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 42874 - optimizer step skipped, scale adjusted\n",
      "Step 43000 | Batch 6856/12048 | Train Loss: 0.7263\n",
      "[Info] Gradient overflow at step 43077 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 43111 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 43303 - optimizer step skipped, scale adjusted\n",
      "Step 43500 | Batch 7356/12048 | Train Loss: 0.6933\n",
      "[Info] Gradient overflow at step 43505 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 43606 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 43612 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 43814 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 43843 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 43972 - optimizer step skipped, scale adjusted\n",
      "Step 44000 | Batch 7856/12048 | Train Loss: 0.7761\n",
      "[Info] Gradient overflow at step 44173 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 44254 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 44457 - optimizer step skipped, scale adjusted\n",
      "Step 44500 | Batch 8356/12048 | Train Loss: 0.7375\n",
      "[Info] Gradient overflow at step 44558 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 44563 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 44665 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 44868 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 44971 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 44978 - optimizer step skipped, scale adjusted\n",
      "Step 45000 | Batch 8856/12048 | Train Loss: 0.7783\n",
      "[Info] Gradient overflow at step 45080 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 45283 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 45384 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 45485 - optimizer step skipped, scale adjusted\n",
      "Step 45500 | Batch 9356/12048 | Train Loss: 0.5937\n",
      "[Info] Gradient overflow at step 45586 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 45688 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 45790 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 45891 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 45979 - optimizer step skipped, scale adjusted\n",
      "Step 46000 | Batch 9856/12048 | Train Loss: 0.8036\n",
      "[Info] Gradient overflow at step 46180 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 46224 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 46426 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 46479 - optimizer step skipped, scale adjusted\n",
      "Step 46500 | Batch 10356/12048 | Train Loss: 0.7410\n",
      "[Info] Gradient overflow at step 46681 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 46782 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 46886 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 46987 - optimizer step skipped, scale adjusted\n",
      "Step 47000 | Batch 10856/12048 | Train Loss: 0.8399\n",
      "[Info] Gradient overflow at step 47092 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 47194 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 47282 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 47485 - optimizer step skipped, scale adjusted\n",
      "Step 47500 | Batch 11356/12048 | Train Loss: 0.7279\n",
      "[Info] Gradient overflow at step 47589 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 47624 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 47825 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 47834 - optimizer step skipped, scale adjusted\n",
      "Step 48000 | Batch 11856/12048 | Train Loss: 0.6738\n",
      "[Info] Gradient overflow at step 48035 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 48137 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 48191 - optimizer step skipped, scale adjusted\n",
      "\n",
      "============================================================\n",
      "End-of-Epoch 4 Validation\n",
      "============================================================\n",
      "\n",
      "--- Epoch 4 Summary ---\n",
      "Train Loss: 0.7577\n",
      "Val Loss: 0.7253\n",
      "ROUGE-1: 0.1681, ROUGE-2: 0.0494, ROUGE-L: 0.1305\n",
      "Elapsed Time: 350.0 minutes\n",
      "============================================================\n",
      "\n",
      "New best model saved! rougeL: 0.1305 (epoch 3)\n",
      "\n",
      "============================================================\n",
      "Epoch 5/5\n",
      "============================================================\n",
      "[Info] Gradient overflow at step 48393 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 48492 - optimizer step skipped, scale adjusted\n",
      "Step 48500 | Batch 308/12048 | Train Loss: 0.7908\n",
      "[Info] Gradient overflow at step 48619 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 48826 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 48929 - optimizer step skipped, scale adjusted\n",
      "Step 49000 | Batch 808/12048 | Train Loss: 0.6716\n",
      "[Info] Gradient overflow at step 49035 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 49136 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 49153 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 49279 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 49481 - optimizer step skipped, scale adjusted\n",
      "Step 49500 | Batch 1308/12048 | Train Loss: 0.6751\n",
      "[Info] Gradient overflow at step 49586 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 49688 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 49792 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 49873 - optimizer step skipped, scale adjusted\n",
      "Step 50000 | Batch 1808/12048 | Train Loss: 0.7221\n",
      "[Info] Gradient overflow at step 50074 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 50176 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 50278 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 50348 - optimizer step skipped, scale adjusted\n",
      "Step 50500 | Batch 2308/12048 | Train Loss: 0.6759\n",
      "[Info] Gradient overflow at step 50553 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 50643 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 50844 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 50922 - optimizer step skipped, scale adjusted\n",
      "Step 51000 | Batch 2808/12048 | Train Loss: 0.6378\n",
      "[Info] Gradient overflow at step 51123 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 51126 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 51329 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 51436 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 51473 - optimizer step skipped, scale adjusted\n",
      "Step 51500 | Batch 3308/12048 | Train Loss: 0.7507\n",
      "[Info] Gradient overflow at step 51674 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 51775 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 51876 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 51980 - optimizer step skipped, scale adjusted\n",
      "Step 52000 | Batch 3808/12048 | Train Loss: 0.7589\n",
      "[Info] Gradient overflow at step 52081 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 52182 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 52284 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 52387 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 52488 - optimizer step skipped, scale adjusted\n",
      "Step 52500 | Batch 4308/12048 | Train Loss: 0.7407\n",
      "[Info] Gradient overflow at step 52589 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 52646 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 52851 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 52943 - optimizer step skipped, scale adjusted\n",
      "Step 53000 | Batch 4808/12048 | Train Loss: 0.7377\n",
      "[Info] Gradient overflow at step 53144 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 53188 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 53389 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 53490 - optimizer step skipped, scale adjusted\n",
      "Step 53500 | Batch 5308/12048 | Train Loss: 0.6416\n",
      "[Info] Gradient overflow at step 53522 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 53651 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 53787 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 53977 - optimizer step skipped, scale adjusted\n",
      "Step 54000 | Batch 5808/12048 | Train Loss: 0.6874\n",
      "[Info] Gradient overflow at step 54181 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 54283 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 54387 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 54488 - optimizer step skipped, scale adjusted\n",
      "Step 54500 | Batch 6308/12048 | Train Loss: 0.7561\n",
      "[Info] Gradient overflow at step 54508 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 54715 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 54723 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 54927 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 54932 - optimizer step skipped, scale adjusted\n",
      "Step 55000 | Batch 6808/12048 | Train Loss: 0.6909\n",
      "[Info] Gradient overflow at step 55133 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 55234 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 55337 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 55440 - optimizer step skipped, scale adjusted\n",
      "Step 55500 | Batch 7308/12048 | Train Loss: 0.6258\n",
      "[Info] Gradient overflow at step 55543 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 55645 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 55747 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 55849 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 55951 - optimizer step skipped, scale adjusted\n",
      "Step 56000 | Batch 7808/12048 | Train Loss: 0.7631\n",
      "[Info] Gradient overflow at step 56052 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 56161 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 56265 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 56294 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 56498 - optimizer step skipped, scale adjusted\n",
      "Step 56500 | Batch 8308/12048 | Train Loss: 0.8868\n",
      "[Info] Gradient overflow at step 56510 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 56702 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 56906 - optimizer step skipped, scale adjusted\n",
      "Step 57000 | Batch 8808/12048 | Train Loss: 0.6407\n",
      "[Info] Gradient overflow at step 57009 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 57111 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 57214 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 57319 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 57421 - optimizer step skipped, scale adjusted\n",
      "Step 57500 | Batch 9308/12048 | Train Loss: 0.6329\n",
      "[Info] Gradient overflow at step 57502 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 57704 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 57807 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 57911 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 57956 - optimizer step skipped, scale adjusted\n",
      "Step 58000 | Batch 9808/12048 | Train Loss: 0.6170\n",
      "[Info] Gradient overflow at step 58158 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 58259 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 58363 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 58464 - optimizer step skipped, scale adjusted\n",
      "Step 58500 | Batch 10308/12048 | Train Loss: 0.7177\n",
      "[Info] Gradient overflow at step 58504 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 58642 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 58843 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 58945 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 58992 - optimizer step skipped, scale adjusted\n",
      "Step 59000 | Batch 10808/12048 | Train Loss: 0.8052\n",
      "[Info] Gradient overflow at step 59194 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 59198 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 59399 - optimizer step skipped, scale adjusted\n",
      "Step 59500 | Batch 11308/12048 | Train Loss: 0.7292\n",
      "[Info] Gradient overflow at step 59500 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 59599 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 59706 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 59909 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 59926 - optimizer step skipped, scale adjusted\n",
      "Step 60000 | Batch 11808/12048 | Train Loss: 0.6695\n",
      "[Info] Gradient overflow at step 60127 - optimizer step skipped, scale adjusted\n",
      "[Info] Gradient overflow at step 60210 - optimizer step skipped, scale adjusted\n",
      "\n",
      "============================================================\n",
      "End-of-Epoch 5 Validation\n",
      "============================================================\n",
      "\n",
      "--- Epoch 5 Summary ---\n",
      "Train Loss: 0.7349\n",
      "Val Loss: 0.7167\n",
      "ROUGE-1: 0.1692, ROUGE-2: 0.0509, ROUGE-L: 0.1305\n",
      "Elapsed Time: 436.9 minutes\n",
      "============================================================\n",
      "\n",
      "No improvement. Patience: 1/3\n",
      "No improvement. Patience: 2/3\n",
      "\n",
      "============================================================\n",
      "Training Completed!\n",
      "============================================================\n",
      "Best rougeL: 0.1305 (Epoch 3)\n",
      "Total Time: 436.9 minutes\n",
      "Best model saved: ./best_model\n",
      "============================================================\n",
      "\n",
      "--- Model Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Model Training ---\")\n",
    "training_history = train_model(model, train_loader, val_loader, tokenizer)\n",
    "print(\"--- Model Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "383d99e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Evaluation ---\n",
      "Starting evaluation on test set...\n",
      "Processed 10/669 batches\n",
      "Processed 20/669 batches\n",
      "Processed 30/669 batches\n",
      "Processed 40/669 batches\n",
      "Processed 50/669 batches\n",
      "Processed 60/669 batches\n",
      "Processed 70/669 batches\n",
      "Processed 80/669 batches\n",
      "Processed 90/669 batches\n",
      "Processed 100/669 batches\n",
      "Processed 110/669 batches\n",
      "Processed 120/669 batches\n",
      "Processed 130/669 batches\n",
      "Processed 140/669 batches\n",
      "Processed 150/669 batches\n",
      "Processed 160/669 batches\n",
      "Processed 170/669 batches\n",
      "Processed 180/669 batches\n",
      "Processed 190/669 batches\n",
      "Processed 200/669 batches\n",
      "Processed 210/669 batches\n",
      "Processed 220/669 batches\n",
      "Processed 230/669 batches\n",
      "Processed 240/669 batches\n",
      "Processed 250/669 batches\n",
      "Processed 260/669 batches\n",
      "Processed 270/669 batches\n",
      "Processed 280/669 batches\n",
      "Processed 290/669 batches\n",
      "Processed 300/669 batches\n",
      "Processed 310/669 batches\n",
      "Processed 320/669 batches\n",
      "Processed 330/669 batches\n",
      "Processed 340/669 batches\n",
      "Processed 350/669 batches\n",
      "Processed 360/669 batches\n",
      "Processed 370/669 batches\n",
      "Processed 380/669 batches\n",
      "Processed 390/669 batches\n",
      "Processed 400/669 batches\n",
      "Processed 410/669 batches\n",
      "Processed 420/669 batches\n",
      "Processed 430/669 batches\n",
      "Processed 440/669 batches\n",
      "Processed 450/669 batches\n",
      "Processed 460/669 batches\n",
      "Processed 470/669 batches\n",
      "Processed 480/669 batches\n",
      "Processed 490/669 batches\n",
      "Processed 500/669 batches\n",
      "Processed 510/669 batches\n",
      "Processed 520/669 batches\n",
      "Processed 530/669 batches\n",
      "Processed 540/669 batches\n",
      "Processed 550/669 batches\n",
      "Processed 560/669 batches\n",
      "Processed 570/669 batches\n",
      "Processed 580/669 batches\n",
      "Processed 590/669 batches\n",
      "Processed 600/669 batches\n",
      "Processed 610/669 batches\n",
      "Processed 620/669 batches\n",
      "Processed 630/669 batches\n",
      "Processed 640/669 batches\n",
      "Processed 650/669 batches\n",
      "Processed 660/669 batches\n",
      "\n",
      "============================================================\n",
      "Overall Test Results:\n",
      "============================================================\n",
      "ROUGE-1: 0.1700\n",
      "ROUGE-2: 0.0518\n",
      "ROUGE-L: 0.1315\n",
      "\n",
      "============================================================\n",
      "Per-Language Test Results:\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "--- Model Evaluation Finished ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Model Evaluation ---\")\n",
    "eval_results = evaluate_model(model, test_loader, tokenizer)\n",
    "overall_rouge = eval_results[\"overall\"]\n",
    "language_rouge = eval_results[\"per_language\"]\n",
    "print(\"--- Model Evaluation Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de00e2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting results...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Expert Usage Summary ---\n",
      "Average overflow rate: 0.00%\n",
      "Max overflow rate: 0.00%\n",
      "Layers with overflow: 0/16\n"
     ]
    }
   ],
   "source": [
    "print(\"Plotting results...\")\n",
    "plot_training_curves(training_history)\n",
    "plot_language_performance(language_rouge)\n",
    "plot_expert_usage(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbaa9cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Model Configuration:\n",
      "- Base Model: google/mt5-small\n",
      "- Languages: english, hindi, punjabi\n",
      "- Number of Experts: 6\n",
      "- Top-K Routing: 3\n",
      "- Batch Size: 32\n",
      "- Learning Rate: 0.0003\n",
      "- Training Epochs: 5\n",
      "\n",
      "Training Results:\n",
      "- Total Training Time: 26212.41 seconds\n",
      "- Final Training Loss: 0.7349\n",
      "- Final Validation Loss: 0.7167\n",
      "\n",
      "Overall Test Performance (ROUGE Scores):\n",
      "- ROUGE-1: 0.1700\n",
      "- ROUGE-2: 0.0518\n",
      "- ROUGE-L: 0.1315\n",
      "\n",
      "Language-Specific Performance:\n",
      "\n",
      "Dataset Statistics:\n",
      "- Training samples: 385515\n",
      "- Validation samples: 21408\n",
      "- Test samples: 21408\n",
      "- Total parameters: 476,620,256\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"- Base Model: {config.model_name}\")\n",
    "print(f\"- Languages: {', '.join(config.languages)}\")\n",
    "print(f\"- Number of Experts: {config.num_experts}\")\n",
    "print(f\"- Top-K Routing: {config.top_k}\")\n",
    "print(f\"- Batch Size: {config.batch_size}\")\n",
    "print(f\"- Learning Rate: {config.learning_rate}\")\n",
    "print(f\"- Training Epochs: {config.num_epochs}\")\n",
    "\n",
    "print(f\"\\nTraining Results:\")\n",
    "print(\n",
    "    f\"- Total Training Time: {training_history['training_time']:.2f} seconds\")\n",
    "print(f\"- Final Training Loss: {training_history['train_losses'][-1]:.4f}\")\n",
    "print(f\"- Final Validation Loss: {training_history['val_losses'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nOverall Test Performance (ROUGE Scores):\")\n",
    "print(f\"- ROUGE-1: {overall_rouge['rouge1']:.4f}\")\n",
    "print(f\"- ROUGE-2: {overall_rouge['rouge2']:.4f}\")\n",
    "print(f\"- ROUGE-L: {overall_rouge['rougeL']:.4f}\")\n",
    "\n",
    "print(f\"\\nLanguage-Specific Performance:\")\n",
    "for lang, scores in language_rouge.items():\n",
    "    print(\n",
    "        f\"- {lang.upper()}: ROUGE-1={scores['rouge1']:.4f}, ROUGE-2={scores['rouge2']:.4f}, ROUGE-L={scores['rougeL']:.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"- Training samples: {len(train_data)}\")\n",
    "print(f\"- Validation samples: {len(val_data)}\")\n",
    "print(f\"- Test samples: {len(test_data)}\")\n",
    "print(\n",
    "    f\"- Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bf9d153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Best Model for Sample Inference ---\n",
      "Loading best model from ./best_model/best_model.pt\n",
      "Best model loaded successfully!\n",
      "Best rougeL score: 0.1305\n",
      "From epoch: 3\n",
      "\n",
      "Performing sample inferences with the best loaded model...\n",
      "\n",
      "[Error] Failed to generate summary for english: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`\n",
      "\n",
      "[Error] Unexpected error during inference: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_179084/396035318.py\", line 46, in <module>\n",
      "    generated_ids = model_to_generate.base_model.generate(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 1597, in generate\n",
      "    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 523, in _prepare_encoder_decoder_kwargs_for_generation\n",
      "    model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\n",
      "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/mt5/modeling_mt5.py\", line 1093, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/mt5/modeling_mt5.py\", line 564, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      "                             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/mt5/modeling_mt5.py\", line 469, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      "                       ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/mt5/modeling_mt5.py\", line 439, in forward\n",
      "    attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_179084/396035318.py\", line 65, in <module>\n",
      "    torch.cuda.empty_cache()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py\", line 162, in empty_cache\n",
      "    torch._C._cuda_emptyCache()\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Loading Best Model for Sample Inference ---\")\n",
    "\n",
    "# Clear CUDA cache first to avoid errors\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "try:\n",
    "    loaded_checkpoint = load_best_model(model, tokenizer)\n",
    "    if loaded_checkpoint:\n",
    "        print(\"\\nPerforming sample inferences with the best loaded model...\")\n",
    "        model.eval()\n",
    "\n",
    "        sample_texts = {\n",
    "            \"english\": \"The quick brown fox jumps over the lazy dog, showcasing its agility and speed. This particular sentence is not just a whimsical phrase; it is a classic pangram that contains every letter of the English alphabet at least once. It has been used for decades to test typewriters, keyboards, and fonts.\",\n",
    "            \"hindi\": \"भारत एक महान देश है जिसका इतिहास सहस्राब्दियों पुराना है। यह विभिन्न संस्कृतियों, भाषाओं, और धर्मों का घर है। भारतीय सभ्यता विश्व की प्राचीनतम सभ्यताओं में से एक है और इसने विज्ञान, गणित, कला और दर्शन के क्षेत्र में महत्वपूर्ण योगदान दिया है।\",\n",
    "            \"punjabi\": \"ਪੰਜਾਬ ਭਾਰਤ ਦਾ ਇੱਕ ਪ੍ਰਮੁੱਖ ਰਾਜ ਹੈ ਜੋ ਆਪਣੇ ਹਰੇ-ਭਰੇ ਖੇਤਾਂ ਅਤੇ ਉਪਜਾਊ ਜ਼ਮੀਨ ਲਈ ਜਾਣਿਆ ਜਾਂਦਾ ਹੈ। ਇੱਥੇ ਦੇ ਲੋਕ ਆਪਣੀ ਮਿਹਨਤ, ਖੁਸ਼ਮਿਜ਼ਾਜੀ ਅਤੇ ਗਰਮਜੋਸ਼ੀ ਲਈ ਮਸ਼ਹੂਰ ਹਨ। ਪੰਜਾਬੀ ਸੱਭਿਆਚਾਰ ਸੰਗੀਤ, ਨਾਚ ਅਤੇ ਭੋਜਨ ਦੇ ਲਈ ਵਿਸ਼ਵ ਭਰ ਵਿੱਚ ਪ੍ਰਸਿੱਧ ਹੈ।\",\n",
    "        }\n",
    "\n",
    "        lang_token_map = {\"english\": \"<en>\", \"hindi\": \"<hi>\", \"punjabi\": \"<pa>\"}\n",
    "        lang_id_map = {\"english\": \"en\", \"hindi\": \"hi\", \"punjabi\": \"pa\"}\n",
    "        \n",
    "        # Extract model for generation (handle DataParallel wrapper)\n",
    "        model_to_generate = model.module if hasattr(model, \"module\") else model\n",
    "\n",
    "        for lang, text in sample_texts.items():\n",
    "            try:\n",
    "                input_text_with_token = f\"{lang_token_map[lang]} summarize: {text}\"\n",
    "\n",
    "                input_encoding = tokenizer(\n",
    "                    input_text_with_token,\n",
    "                    max_length=config.max_input_length,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "\n",
    "                input_ids = input_encoding.input_ids.to(device)\n",
    "                attention_mask = input_encoding.attention_mask.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Use beam search for quality (config.num_beams)\n",
    "                    # Set language_ids for proper MoE routing\n",
    "                    model_to_generate.current_language_ids = [lang_id_map[lang]]\n",
    "                    \n",
    "                    generated_ids = model_to_generate.base_model.generate(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=config.max_target_length,\n",
    "                        num_beams=config.num_beams,  # Use config beam search setting\n",
    "                        early_stopping=True,\n",
    "                    )\n",
    "\n",
    "                summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                print(f\"\\n--- Sample Inference for {lang.upper()} ---\")\n",
    "                print(f\"Input: {text[:100]}...\")  # Show first 100 chars\n",
    "                print(f\"Generated Summary: {summary}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n[Error] Failed to generate summary for {lang}: {str(e)}\")\n",
    "                # Clear CUDA cache on error\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                continue\n",
    "                \n",
    "    else:\n",
    "        print(\"\\nCould not load the best model for sample inference.\")\n",
    "        print(\"This may be because training hasn't been run yet or checkpoint is missing.\")\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n[Error] Checkpoint file not found: {str(e)}\")\n",
    "    print(\"Please run the training cells first to generate a checkpoint.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[Error] Unexpected error during inference: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81d20fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ continue_training() function loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [3,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [0,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [2,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [1,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    }
   ],
   "source": [
    "def continue_training(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    tokenizer, \n",
    "    additional_epochs, \n",
    "    checkpoint_path=None,\n",
    "    new_learning_rate=None,\n",
    "    reset_optimizer=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Continue training from a checkpoint for additional epochs.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to continue training\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        tokenizer: Tokenizer for decoding\n",
    "        additional_epochs: Number of additional epochs to train\n",
    "        checkpoint_path: Path to checkpoint to resume from (default: best_model/best_model.pt)\n",
    "        new_learning_rate: Optional new learning rate (default: use config.learning_rate)\n",
    "        reset_optimizer: If True, creates new optimizer instead of loading from checkpoint\n",
    "    \n",
    "    Returns:\n",
    "        Updated training_history dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load checkpoint if path provided, otherwise use best model\n",
    "    if checkpoint_path is None:\n",
    "        checkpoint_path = os.path.join(config.checkpoint_dir, \"best_model\", \"best_model.pt\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CONTINUING TRAINING FOR {additional_epochs} MORE EPOCHS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    \n",
    "    # Get previous training history\n",
    "    previous_history = checkpoint.get(\"training_history\", {\n",
    "        \"train_losses\": [],\n",
    "        \"val_losses\": [],\n",
    "        \"val_rouge_scores\": [],\n",
    "        \"training_time\": 0\n",
    "    })\n",
    "    \n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    \n",
    "    print(f\"Previous training: {checkpoint['epoch'] + 1} epochs completed\")\n",
    "    print(f\"Best ROUGE-L so far: {checkpoint['rouge_scores']['rougeL']:.4f}\")\n",
    "    print(f\"Previous training time: {previous_history.get('training_time', 0)/60:.1f} minutes\")\n",
    "    \n",
    "    # Setup optimizer\n",
    "    if reset_optimizer or new_learning_rate:\n",
    "        lr = new_learning_rate if new_learning_rate else config.learning_rate\n",
    "        print(f\"\\nCreating new optimizer with LR={lr:.2e}\")\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=config.weight_decay)\n",
    "    else:\n",
    "        print(f\"\\nRestoring optimizer from checkpoint\")\n",
    "        optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    \n",
    "    # Setup scheduler for additional epochs\n",
    "    total_steps = len(train_loader) * additional_epochs\n",
    "    # Reduced warmup for continued training (10% of original)\n",
    "    warmup_steps = max(100, config.warmup_steps // 10)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    print(f\"Total additional steps: {total_steps}\")\n",
    "    print(f\"Warmup steps: {warmup_steps}\")\n",
    "    \n",
    "    # Initialize GradScaler\n",
    "    scaler = torch.cuda.amp.GradScaler(\n",
    "        init_scale=2**10,\n",
    "        growth_factor=2.0,\n",
    "        backoff_factor=0.5,\n",
    "        growth_interval=100\n",
    "    )\n",
    "    \n",
    "    # Initialize checkpoint manager\n",
    "    checkpoint_manager = ModelCheckpoint(\n",
    "        checkpoint_dir=config.checkpoint_dir,\n",
    "        save_best_only=config.save_best_only,\n",
    "        best_metric=config.best_metric,\n",
    "        patience=config.patience,\n",
    "        min_delta=config.min_delta,\n",
    "    )\n",
    "    \n",
    "    # Restore checkpoint manager state\n",
    "    checkpoint_manager.best_score = checkpoint.get(\"best_score\", checkpoint[\"rouge_scores\"][config.best_metric])\n",
    "    checkpoint_manager.best_epoch = checkpoint.get(\"best_epoch\", checkpoint[\"epoch\"])\n",
    "    \n",
    "    # Copy previous history\n",
    "    train_losses = previous_history[\"train_losses\"].copy()\n",
    "    val_losses = previous_history[\"val_losses\"].copy()\n",
    "    val_rouge_scores = previous_history[\"val_rouge_scores\"].copy()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Starting continued training...\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(start_epoch, start_epoch + additional_epochs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{start_epoch + additional_epochs} (Overall Epoch {epoch+1})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batches_processed = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            languages = batch[\"language\"]\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                    language_ids=languages\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                if loss.dim() > 0:\n",
    "                    loss = loss.mean()\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_val)\n",
    "            scaler.step(optimizer)\n",
    "            scale = scaler.get_scale()\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Check for gradient overflow\n",
    "            skip_step = (scaler.get_scale() < scale)\n",
    "            if skip_step and global_step > warmup_steps:\n",
    "                print(f\"[Info] Gradient overflow at step {global_step} - optimizer step skipped\")\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            batches_processed += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            # Log training progress\n",
    "            if global_step % config.LOG_INTERVALS == 0:\n",
    "                print(f\"Step {global_step} | Batch {batch_idx+1}/{len(train_loader)} | Train Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = epoch_loss / max(batches_processed, 1)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # End-of-epoch validation\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"End-of-Epoch {epoch+1} Validation\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        model.eval()\n",
    "        eoe_val_loss = 0\n",
    "        eoe_val_predictions = []\n",
    "        eoe_val_references = []\n",
    "        eoe_batches_processed = 0\n",
    "        eoe_generation_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                languages = batch[\"language\"]\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels,\n",
    "                        language_ids=languages\n",
    "                    )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                if loss.dim() > 0:\n",
    "                    loss = loss.mean()\n",
    "                eoe_val_loss += loss.item()\n",
    "                eoe_batches_processed += 1\n",
    "                \n",
    "                # Generate predictions for ROUGE\n",
    "                if eoe_generation_batches < config.max_val_generation_batches:\n",
    "                    pred_texts, ref_texts = generate_and_decode(\n",
    "                        model, input_ids, attention_mask, labels, tokenizer\n",
    "                    )\n",
    "                    \n",
    "                    if pred_texts is not None:\n",
    "                        eoe_val_predictions.extend(pred_texts)\n",
    "                        eoe_val_references.extend(ref_texts)\n",
    "                    eoe_generation_batches += 1\n",
    "        \n",
    "        avg_eoe_val_loss = eoe_val_loss / max(eoe_batches_processed, 1)\n",
    "        val_losses.append(avg_eoe_val_loss)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        eoe_rouge_scores = calculate_rouge_scores(\n",
    "            eoe_val_predictions, eoe_val_references\n",
    "        ) if eoe_val_predictions else {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "        \n",
    "        val_rouge_scores.append(eoe_rouge_scores)\n",
    "        \n",
    "        # Update training history\n",
    "        total_training_time = previous_history.get(\"training_time\", 0) + (time.time() - start_time)\n",
    "        training_history = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"val_rouge_scores\": val_rouge_scores,\n",
    "            \"training_time\": total_training_time,\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1} Summary ---\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {avg_eoe_val_loss:.4f}\")\n",
    "        print(f\"ROUGE-1: {eoe_rouge_scores['rouge1']:.4f}, ROUGE-2: {eoe_rouge_scores['rouge2']:.4f}, ROUGE-L: {eoe_rouge_scores['rougeL']:.4f}\")\n",
    "        print(f\"Elapsed Time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "        print(f\"Total Training Time: {total_training_time/60:.1f} minutes\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            epoch,\n",
    "            avg_train_loss,\n",
    "            avg_eoe_val_loss,\n",
    "            eoe_rouge_scores,\n",
    "            training_history,\n",
    "        )\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if checkpoint_manager.should_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Update final metadata\n",
    "    final_training_time = previous_history.get(\"training_time\", 0) + (time.time() - start_time)\n",
    "    training_history[\"training_time\"] = final_training_time\n",
    "    training_history[\"training_complete\"] = True\n",
    "    \n",
    "    metadata = {\n",
    "        \"best_score\": checkpoint_manager.best_score,\n",
    "        \"best_epoch\": checkpoint_manager.best_epoch,\n",
    "        \"final_epoch\": epoch,\n",
    "        \"training_complete\": True,\n",
    "        \"total_training_time\": final_training_time,\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(config.checkpoint_dir, \"training_metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Continued Training Completed!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Additional epochs trained: {additional_epochs}\")\n",
    "    print(f\"Total epochs: {epoch + 1}\")\n",
    "    print(f\"Best {config.best_metric}: {checkpoint_manager.best_score:.4f} (Epoch {checkpoint_manager.best_epoch + 1})\")\n",
    "    print(f\"Additional training time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    print(f\"Total training time: {final_training_time/60:.1f} minutes\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "print(\"✅ continue_training() function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca880fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ reevaluate_and_plot() function loaded\n"
     ]
    }
   ],
   "source": [
    "def reevaluate_and_plot(model, test_loader, tokenizer, training_history):\n",
    "    \"\"\"\n",
    "    Re-evaluate the model on test set and plot updated training curves.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        test_loader: Test data loader\n",
    "        tokenizer: Tokenizer for decoding\n",
    "        training_history: Updated training history dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with overall and per-language ROUGE scores\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RE-EVALUATING MODEL ON TEST SET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load best model\n",
    "    best_model_path = os.path.join(config.checkpoint_dir, \"best_model\", \"best_model.pt\")\n",
    "    if os.path.exists(best_model_path):\n",
    "        checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        print(f\"Loaded best model from epoch {checkpoint['epoch'] + 1}\")\n",
    "        print(f\"Best ROUGE-L: {checkpoint['rouge_scores']['rougeL']:.4f}\")\n",
    "    else:\n",
    "        print(\"Warning: Best model not found, using current model state\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    eval_results = evaluate_model(model, test_loader, tokenizer)\n",
    "    overall_rouge = eval_results[\"overall\"]\n",
    "    language_rouge = eval_results[\"per_language\"]\n",
    "    \n",
    "    # Plot updated training curves\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PLOTTING UPDATED TRAINING CURVES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    plot_training_curves(training_history)\n",
    "    plot_language_performance(language_rouge)\n",
    "    plot_expert_usage(model)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal Training Epochs: {len(training_history['train_losses'])}\")\n",
    "    print(f\"Total Training Time: {training_history['training_time']/3600:.2f} hours\")\n",
    "    \n",
    "    print(f\"\\nOverall Test Performance:\")\n",
    "    print(f\"  ROUGE-1: {overall_rouge['rouge1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {overall_rouge['rouge2']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {overall_rouge['rougeL']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Language Performance:\")\n",
    "    for lang, scores in language_rouge.items():\n",
    "        print(f\"  {lang.upper()}: R-1={scores['rouge1']:.4f}, R-2={scores['rouge2']:.4f}, R-L={scores['rougeL']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest Validation ROUGE-L: {max([s['rougeL'] for s in training_history['val_rouge_scores']]):.4f}\")\n",
    "    print(f\"Final Validation ROUGE-L: {training_history['val_rouge_scores'][-1]['rougeL']:.4f}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "print(\"✅ reevaluate_and_plot() function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a43e158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ run_sample_inference_updated() function loaded\n"
     ]
    }
   ],
   "source": [
    "def run_sample_inference_updated(model, tokenizer, sample_texts=None):\n",
    "    \"\"\"\n",
    "    Run sample inference with the current model state.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        tokenizer: Tokenizer for encoding/decoding\n",
    "        sample_texts: Optional dictionary of sample texts by language\n",
    "                     If None, uses default samples\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of generated summaries by language\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING SAMPLE INFERENCE WITH UPDATED MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Load best model\n",
    "    best_model_path = os.path.join(config.checkpoint_dir, \"best_model\", \"best_model.pt\")\n",
    "    if os.path.exists(best_model_path):\n",
    "        checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        print(f\"Loaded best model from epoch {checkpoint['epoch'] + 1}\")\n",
    "        print(f\"Best ROUGE-L: {checkpoint['rouge_scores']['rougeL']:.4f}\\n\")\n",
    "    \n",
    "    # Default sample texts if not provided\n",
    "    if sample_texts is None:\n",
    "        sample_texts = {\n",
    "            \"english\": \"The quick brown fox jumps over the lazy dog, showcasing its agility and speed. This particular sentence is not just a whimsical phrase; it is a classic pangram that contains every letter of the English alphabet at least once. It has been used for decades to test typewriters, keyboards, and fonts.\",\n",
    "            \"hindi\": \"भारत एक महान देश है जिसका इतिहास सहस्राब्दियों पुराना है। यह विभिन्न संस्कृतियों, भाषाओं, और धर्मों का घर है। भारतीय सभ्यता विश्व की प्राचीनतम सभ्यताओं में से एक है और इसने विज्ञान, गणित, कला और दर्शन के क्षेत्र में महत्वपूर्ण योगदान दिया है।\",\n",
    "            \"punjabi\": \"ਪੰਜਾਬ ਭਾਰਤ ਦਾ ਇੱਕ ਪ੍ਰਮੁੱਖ ਰਾਜ ਹੈ ਜੋ ਆਪਣੇ ਹਰੇ-ਭਰੇ ਖੇਤਾਂ ਅਤੇ ਉਪਜਾਊ ਜ਼ਮੀਨ ਲਈ ਜਾਣਿਆ ਜਾਂਦਾ ਹੈ। ਇੱਥੇ ਦੇ ਲੋਕ ਆਪਣੀ ਮਿਹਨਤ, ਖੁਸ਼ਮਿਜ਼ਾਜੀ ਅਤੇ ਗਰਮਜੋਸ਼ੀ ਲਈ ਮਸ਼ਹੂਰ ਹਨ। ਪੰਜਾਬੀ ਸੱਭਿਆਚਾਰ ਸੰਗੀਤ, ਨਾਚ ਅਤੇ ਭੋਜਨ ਦੇ ਲਈ ਵਿਸ਼ਵ ਭਰ ਵਿੱਚ ਪ੍ਰਸਿੱਧ ਹੈ।\",\n",
    "        }\n",
    "    \n",
    "    lang_token_map = {\"english\": \"<en>\", \"hindi\": \"<hi>\", \"punjabi\": \"<pa>\"}\n",
    "    lang_id_map = {\"english\": \"en\", \"hindi\": \"hi\", \"punjabi\": \"pa\"}\n",
    "    \n",
    "    model.eval()\n",
    "    model_to_generate = model.module if hasattr(model, \"module\") else model\n",
    "    \n",
    "    generated_summaries = {}\n",
    "    \n",
    "    for lang, text in sample_texts.items():\n",
    "        try:\n",
    "            input_text_with_token = f\"{lang_token_map[lang]} summarize: {text}\"\n",
    "            \n",
    "            input_encoding = tokenizer(\n",
    "                input_text_with_token,\n",
    "                max_length=config.max_input_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            input_ids = input_encoding.input_ids.to(device)\n",
    "            attention_mask = input_encoding.attention_mask.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Set language_ids for proper MoE routing\n",
    "                model_to_generate.current_language_ids = [lang_id_map[lang]]\n",
    "                \n",
    "                generated_ids = model_to_generate.base_model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=config.max_target_length,\n",
    "                    num_beams=config.num_beams,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "            \n",
    "            summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            generated_summaries[lang] = summary\n",
    "            \n",
    "            print(f\"\\n--- Sample Inference for {lang.upper()} ---\")\n",
    "            print(f\"Input: {text[:100]}...\")\n",
    "            print(f\"Generated Summary: {summary}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[Error] Failed to generate summary for {lang}: {str(e)}\")\n",
    "            generated_summaries[lang] = None\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    return generated_summaries\n",
    "\n",
    "print(\"✅ run_sample_inference_updated() function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81022ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Continue training for N more epochs (uncomment to run)\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Continue training for 2 more epochs with same settings\n",
    "# Uncomment to run:\n",
    "\n",
    "# training_history = continue_training(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     tokenizer=tokenizer,\n",
    "#     additional_epochs=2,\n",
    "#     checkpoint_path=None,  # Uses best model by default\n",
    "#     new_learning_rate=None,  # Uses config.learning_rate\n",
    "#     reset_optimizer=False  # Continue with saved optimizer state\n",
    "# )\n",
    "\n",
    "print(\"Example 1: Continue training for N more epochs (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d2f03bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2: Fine-tune with lower learning rate (uncomment to run)\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Continue training with a lower learning rate (fine-tuning)\n",
    "# Uncomment to run:\n",
    "\n",
    "# training_history = continue_training(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     tokenizer=tokenizer,\n",
    "#     additional_epochs=3,\n",
    "#     checkpoint_path=None,\n",
    "#     new_learning_rate=1e-4,  # Lower LR for fine-tuning (was 3e-4)\n",
    "#     reset_optimizer=True  # Start with fresh optimizer\n",
    "# )\n",
    "\n",
    "print(\"Example 2: Fine-tune with lower learning rate (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b070b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3: Re-evaluate and plot results (uncomment to run)\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Re-evaluate and plot after continued training\n",
    "# Uncomment to run:\n",
    "\n",
    "# eval_results = reevaluate_and_plot(\n",
    "#     model=model,\n",
    "#     test_loader=test_loader,\n",
    "#     tokenizer=tokenizer,\n",
    "#     training_history=training_history\n",
    "# )\n",
    "# \n",
    "# overall_rouge = eval_results[\"overall\"]\n",
    "# language_rouge = eval_results[\"per_language\"]\n",
    "\n",
    "print(\"Example 3: Re-evaluate and plot results (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51c4a7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 4: Run sample inference (uncomment to run)\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Run sample inference with updated model\n",
    "# Uncomment to run:\n",
    "\n",
    "# summaries = run_sample_inference_updated(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     sample_texts=None  # Uses default samples, or provide your own dict\n",
    "# )\n",
    "# \n",
    "# # Access individual summaries\n",
    "# # print(f\"English summary: {summaries['english']}\")\n",
    "# # print(f\"Hindi summary: {summaries['hindi']}\")\n",
    "# # print(f\"Punjabi summary: {summaries['punjabi']}\")\n",
    "\n",
    "print(\"Example 4: Run sample inference (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dbb61a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 Uncomment the code block above to continue training for additional epochs\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================\n",
    "# QUICK START: Uncomment the block below to continue training for N more epochs\n",
    "# ==================================================================================\n",
    "\n",
    "# # STEP 1: Continue training (adjust additional_epochs as needed)\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"STARTING CONTINUED TRAINING\")\n",
    "# print(\"=\"*80 + \"\\n\")\n",
    "# \n",
    "# training_history = continue_training(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     tokenizer=tokenizer,\n",
    "#     additional_epochs=2,  # ← Change this to train for more/fewer epochs\n",
    "#     checkpoint_path=None,  # Uses best model automatically\n",
    "#     new_learning_rate=None,  # Set to 1e-4 for fine-tuning, or None to keep original\n",
    "#     reset_optimizer=False  # Set True if using new_learning_rate\n",
    "# )\n",
    "# \n",
    "# # STEP 2: Re-evaluate and plot\n",
    "# eval_results = reevaluate_and_plot(\n",
    "#     model=model,\n",
    "#     test_loader=test_loader,\n",
    "#     tokenizer=tokenizer,\n",
    "#     training_history=training_history\n",
    "# )\n",
    "# \n",
    "# # STEP 3: Run sample inference\n",
    "# summaries = run_sample_inference_updated(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer\n",
    "# )\n",
    "# \n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"✅ CONTINUED TRAINING COMPLETE!\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "print(\"💡 Uncomment the code block above to continue training for additional epochs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
